from data.load_dataset import load_dataset
from data.merge_dataset import merge_dataset
from data.feature_engineering import *
from model.model_train import *
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import PolynomialFeatures
import torch
import seaborn as sns
from matplotlib.ticker import ScalarFormatter
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import matplotlib.pyplot as plt
import warnings
import random

warnings.filterwarnings("ignore", category=UserWarning)
random.seed(42)
np.random.seed(42)
torch.manual_seed(42)
if torch.cuda.is_available():
    torch.cuda.manual_seed(42)
    torch.cuda.manual_seed_all(42)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

match torch.cuda.is_available():
    case True: device = "cuda"
    case _: device = "cpu"

# ë°ì´í„° ë¡œë“œ
train_data, test_data, sample_submission, interest_data, subway_data, school_data, park_data = load_dataset()
train_data, test_data = merge_dataset(train_data, test_data, interest_data, subway_data, school_data, park_data)


### Data Preprocessing

# ìœ„ì¹˜ ì¤‘ë³µë„ ë‚®ì€ í–‰ ì‚­ì œ
groups = train_data.groupby(["latitude", "longitude"])["index"].count()
conditioned_groups_index = groups[(groups >= 2) & (groups <= 5)].index # ì´ ë²”ìœ„ë¥¼ íŒŒë¼ë¯¸í„°ë¡œ ì¡°ì •í•˜ëŠ”ê±¸ë¡œ
small_groups = train_data[
    train_data["latitude"].isin(conditioned_groups_index.get_level_values(0)) &
    train_data["longitude"].isin(conditioned_groups_index.get_level_values(1))
]
train_data.drop(small_groups.index, axis=0, inplace=True)

# built_year > 2024 í–‰ ì‚­ì œ
train_data = train_data[train_data["built_year"] < 2024]
train_data.reset_index(drop=True, inplace=True)

# log ë³€í™˜
train_data, test_data = apply_log_transformation(train_data, test_data)

# year, month ë³€ìˆ˜ ë¶„ë¦¬
train_data["contract_year"] = train_data["contract_year_month"] // 100
train_data["contract_month"] = train_data["contract_year_month"] % 100
test_data["contract_year"] = test_data["contract_year_month"] // 100
test_data["contract_month"] = test_data["contract_year_month"] % 100


### Feature Selection

selected_cols = [
    "area_m2",
    "built_year",
    "latitude",
    "longitude",
    "log_leader_distance",
    "log_subway_distance",
    # "log_school_distance",
    "log_park_distance",
    # "contract_year_month",
    "num_of_subways_within_radius",
    "park",
    "region",
    # ì¶”ê°€ ë³€ìˆ˜
    # "interest_rate",
    # "num_of_schools_within_radius",
    # "num_of_parks_within_radius",
    # "contract_type",
    "contract_year",
    "contract_month"
]

X_train, y_train = train_data[selected_cols], train_data["deposit"]
X_test = test_data[selected_cols]
log_y_train = train_data["log_deposit"]

# StandardScaler ì ìš©
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train, y_train)
X_test = scaler.fit_transform(X_test)

# ë‹¤í•­ì‹ ë³€í™˜ ì ìš© (2ì°¨ ë‹¤í•­ì‹)
poly = PolynomialFeatures(degree=2, include_bias=False)
X_train = poly.fit_transform(X_train)
X_test = poly.fit_transform(X_test)

# í›ˆë ¨, ê²€ì¦ ë°ì´í„° ë¶„ë¦¬
X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2)

# ìë£Œí˜• ë³€ê²½
X_train = torch.tensor(X_train, dtype=torch.float32, device=device)
y_train = torch.tensor(y_train.values, dtype=torch.float32, device=device)
X_val = torch.tensor(X_valid, dtype=torch.float32, device=device)
y_val = torch.tensor(y_valid.values, dtype=torch.float32, device=device)
X_test = torch.tensor(X_test, dtype=torch.float32, device=device)

# ë°ì´í„°ë¡œë” ì„¤ì •
train_dataset = TensorDataset(X_train, y_train)
val_dataset = TensorDataset(X_val, y_val)
train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False)

### MLP ëª¨ë¸ ì •ì˜

input_size = X_train.shape[1]

class SimpleModel(nn.Module):
    def __init__(self):
        super(SimpleModel, self).__init__()
        self.fc1 = nn.Linear(input_size, 64)
        self.fc2 = nn.Linear(64, 16)
        self.fc3 = nn.Linear(16, 1)
        self.relu = nn.ReLU()
        self.softplus = nn.Softplus()
        self.dropout = nn.Dropout(p=0.2)

    def forward(self, x):
        x = self.relu(self.fc1(x))
        x = self.dropout(x)
        x = self.relu(self.fc2(x))
        x = self.dropout(x)
        out = self.softplus(self.fc3(x))
        return out


### ëª¨ë¸ í•™ìŠµ

# ëª¨ë¸ ê°ì²´ ìƒì„±
model = SimpleModel()
model.to(device)

# ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”
for layer in model.children():
    if isinstance(layer, nn.Linear):
        torch.nn.init.xavier_uniform_(layer.weight)

# ì†ì‹¤ í•¨ìˆ˜ì™€ ì˜µí‹°ë§ˆì´ì € ì •ì˜
criterion = nn.L1Loss()  # Mean Absolute Error Loss
optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adam ì˜µí‹°ë§ˆì´ì €
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)

# ì¡°ê¸° ì¢…ë£Œë¥¼ ìœ„í•œ íŒŒë¼ë¯¸í„° ì„¤ì •
patience = 20
best_val_loss = float('inf')
epochs_no_improve = 0

# í•™ìŠµ ê³¼ì •
num_epochs = 1000
loss_df = []
for epoch in range(num_epochs): # tqdm(range(num_epochs), desc="ğŸ’ƒTotal EpochğŸ•º"):
    model.train()
    epoch_loss = 0
    for inputs, targets in train_loader: # tqdm(train_loader, desc=f"âœ¨Epoch {epoch+1}âœ¨:", leave=False):
        optimizer.zero_grad()
        outputs = model(inputs).squeeze()
        loss = criterion(outputs, targets)
        loss.backward()
        optimizer.step()
        epoch_loss += loss.item()

    average_loss = epoch_loss / len(train_loader)
    scheduler.step(average_loss)
    print(f'âœ¨Epoch [{epoch+1}/{num_epochs}]âœ¨ Train Loss: {average_loss:.4f}', end=', ')

    # ê²€ì¦ ê³¼ì •
    model.eval()
    val_epoch_loss = 0
    with torch.no_grad():
        for inputs, targets in val_loader:
            outputs = model(inputs).squeeze()
            val_loss = criterion(outputs, targets)
            val_epoch_loss += val_loss.item()

    average_val_loss = val_epoch_loss / len(val_loader)

    # # NaN ì˜ˆì™¸ ì²˜ë¦¬
    # if math.isnan(average_val_loss):
    #     print('Validation Loss is NaN. Skipping this epoch.')
    #     continue
    
    print(f'Valid Loss: {average_val_loss:.4f}')

    # ì¡°ê¸° ì¢…ë£Œ ì¡°ê±´ ì²´í¬
    if average_val_loss < best_val_loss:
        best_val_loss = average_val_loss
        epochs_no_improve = 0  # ì„±ëŠ¥ì´ ê°œì„ ëœ ê²½ìš°, ì¹´ìš´í„°ë¥¼ ë¦¬ì…‹
    else:
        epochs_no_improve += 1  # ì„±ëŠ¥ì´ ê°œì„ ë˜ì§€ ì•Šìœ¼ë©´ ì¹´ìš´í„° ì¦ê°€

    # ì„¤ì •í•œ patienceë¥¼ ì´ˆê³¼í•˜ë©´ í•™ìŠµ ì¤‘ë‹¨
    if epochs_no_improve >= patience:
        print(f"Early stopping triggered after {epoch+1} epochs. Best validation loss: {best_val_loss:.4f}")
        break

    # ì†ì‹¤ ê¸°ë¡
    loss_df.append(average_val_loss)


### ì˜ˆì¸¡, ì‹œê°í™” ë° ì œì¶œ íŒŒì¼ ìƒì„±

# ì—í¬í¬ ë³„ ì†ì‹¤ ì‹œê°í™” (.pyì˜ ê²½ìš° ìƒëµ)
sns.lineplot(loss_df, color="tomato")

plt.grid()
plt.title("MAE Loss")
plt.xlabel("Epoch")
plt.ylabel("Loss")
ax = plt.gca()
ax.yaxis.set_major_formatter(ScalarFormatter(useMathText=True))
ax.ticklabel_format(style='sci', axis='y', scilimits=(0, 0))
plt.show()


# ì˜ˆì¸¡ ìˆ˜í–‰ ë° ë°°ì¹˜ ë‹¨ìœ„ë¡œ MAE ê³„ì‚°
model.eval()
with torch.no_grad():
    y_pred = model(X_test).squeeze()
print(y_pred)


# ì˜ˆì¸¡ê°’ ì‹œê°í™” (.pyì˜ ê²½ìš° ìƒëµ)
y_pred = y_pred.cpu()
y_pred = pd.Series(y_pred)
sns.histplot(y_pred)

# í†µê³„ëŸ‰ í™•ì¸
y_pred.describe()

# ì œì¶œ íŒŒì¼ ìƒì„±
sample_submission["deposit"] = y_pred
sample_submission.to_csv("output.csv", index=False)


### (ì„ íƒ) ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ ì €ì¥

# í˜„ì¬ ëª¨ë¸ ê°€ì¤‘ì¹˜ë¥¼ ì €ì¥
torch.save(model.state_dict(), 'model_checkpoint.pth') # epoch 152ì—ì„œ ì¤‘ë‹¨

# # ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± í›„ ê°€ì¤‘ì¹˜ ë¡œë“œ
# model.load_state_dict(torch.load('model_checkpoint.pth'))
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor
from catboost import CatBoostRegressor
from data.load_dataset import load_dataset
from data.merge_dataset import merge_dataset
from data.feature_engineering import *
from model.inference import save_csv
from model.feature_select import select_features
from model.data_split import split_features_and_target
from model.model_train import *
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import matplotlib.pyplot as plt
from tqdm import tqdm
import argparse
import os
import wandb
import warnings
warnings.filterwarnings("ignore", category=UserWarning)

train_data, test_data, sample_submission, interest_data, subway_data, school_data, park_data = load_dataset()
# ê¸°ì¡´ ë°ì´í„°ì— ìƒˆë¡œìš´ featureë“¤ì„ ë³‘í•©í•œ ë°ì´í„°í”„ë ˆì„ ë¶ˆëŸ¬ì˜¤ê¸°
train_data, test_data = merge_dataset(train_data, test_data, interest_data, subway_data, school_data, park_data)

### 3. Data Preprocessing

# ìœ„ì¹˜ ì¤‘ë³µë„ ë‚®ì€ í–‰ ì‚­ì œ
groups = train_data.groupby(["latitude", "longitude"])["index"].count()
conditioned_groups_index = groups[(groups >= 2) & (groups <= 5)].index # ì´ ë²”ìœ„ë¥¼ íŒŒë¼ë¯¸í„°ë¡œ ì¡°ì •í•˜ëŠ”ê±¸ë¡œ
small_groups = train_data[
    train_data["latitude"].isin(conditioned_groups_index.get_level_values(0)) &
    train_data["longitude"].isin(conditioned_groups_index.get_level_values(1))
]
train_data.drop(small_groups.index, axis=0, inplace=True)

# built_year > 2024 í–‰ ì‚­ì œ
train_data = train_data[train_data["built_year"] < 2024]
train_data.reset_index(drop=True, inplace=True)

# log ë³€í™˜
train_data, test_data = apply_log_transformation(train_data, test_data)

selected_cols = [
    "log_area_m2",
    "built_year",
    "latitude",
    "longitude",
    "log_leader_distance",
    "log_subway_distance",
    "log_school_distance",
    "log_park_distance",
    "contract_year_month",
    # "contract_day",
    "num_of_subways_within_radius",
    "park",
    "region"
]
X_train, y_train = train_data[selected_cols], train_data["deposit"]
X_test = test_data[selected_cols]
log_y_train = train_data["log_deposit"]

# 1. ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
X_tensor = torch.tensor(X_train.values, dtype=torch.float32)
y_tensor = torch.tensor(y_train.values, dtype=torch.float32)
X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)

train_dataset = TensorDataset(X_tensor, y_tensor)
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)

# 2. ëª¨ë¸ ì •ì˜
class SimpleModel(nn.Module):
    def __init__(self):
        super(SimpleModel, self).__init__()
        self.fc1 = nn.Linear(12, 4)
        self.fc2 = nn.Linear(4, 1)
        self.relu = nn.ReLU()

    def forward(self, x):
        out = self.relu(self.fc1(x))
        out = self.relu(self.fc2(out))
        # out = self.dense3(out)
        # out = self.relu(out)
        return out

# 3. ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
model = SimpleModel()

# ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”
for layer in model.children():
    if isinstance(layer, nn.Linear):
        torch.nn.init.xavier_uniform_(layer.weight)

# 4. ì†ì‹¤ í•¨ìˆ˜ì™€ ì˜µí‹°ë§ˆì´ì € ì •ì˜
criterion = nn.L1Loss()  # Mean Absolute Error Loss
optimizer = optim.Adam(model.parameters(), lr=0.0005)  # Adam ì˜µí‹°ë§ˆì´ì €

# 5. ëª¨ë¸ í•™ìŠµ
num_epochs = 5
loss_df = []
for epoch in tqdm(range(num_epochs), desc="ğŸ’ƒTotal EpochğŸ•º"):
    model.train()
    epoch_loss = 0
    for inputs, targets in tqdm(train_loader, desc=f"âœ¨Epoch {epoch+1}âœ¨:"):
        optimizer.zero_grad()
        outputs = model(inputs).squeeze()
        loss = criterion(outputs, targets)
        loss.backward()
        optimizer.step()
        epoch_loss += loss.item()
    
    average_loss = epoch_loss / len(train_loader)
    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {average_loss:.4f}')
    loss_df.append(average_loss)


plt.plot(loss_df)

# 6. ì˜ˆì¸¡ ìˆ˜í–‰ ë° ë°°ì¹˜ ë‹¨ìœ„ë¡œ MAE ê³„ì‚°
model.eval()
with torch.no_grad():
    y_pred = model(X_test_tensor).squeeze()
print(y_pred)
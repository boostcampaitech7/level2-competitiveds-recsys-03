{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from data.load_dataset import load_dataset\n",
    "from data.merge_dataset import merge_dataset\n",
    "from data.feature_engineering import *\n",
    "from model.inference import save_csv\n",
    "from model.feature_select import select_features\n",
    "from model.data_split import split_features_and_target\n",
    "from model.log_transformation import apply_log_transformation\n",
    "from model.model_train import set_model, optuna_train\n",
    "#from model.TreeModel import XGBoost\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "import optuna\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기존 데이터 불러오기\n",
    "train_data, test_data, sample_submission, interest_data, subway_data, school_data, park_data = load_dataset()\n",
    "# 기존 데이터에 새로운 feature들을 병합한 데이터프레임 불러오기\n",
    "train_data, test_data = merge_dataset(train_data, test_data, interest_data, subway_data, school_data, park_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위치 중복도 낮은 행 삭제\n",
    "groups = train_data.groupby([\"latitude\", \"longitude\"])[\"index\"].count()\n",
    "conditioned_groups_index = groups[(groups >= 2) & (groups <= 5)].index # 이 범위를 파라미터로 조정하는걸로\n",
    "small_groups = train_data[\n",
    "    train_data[\"latitude\"].isin(conditioned_groups_index.get_level_values(0)) &\n",
    "    train_data[\"longitude\"].isin(conditioned_groups_index.get_level_values(1))\n",
    "]\n",
    "train_data.drop(small_groups.index, axis=0, inplace=True)\n",
    "\n",
    "# built_year > 2024 행 삭제\n",
    "train_data = train_data[train_data[\"built_year\"] < 2024]\n",
    "train_data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Clustering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 준비\n",
    "feature_columns = [\"latitude\", \"longitude\"]\n",
    "coords = train_data[feature_columns]\n",
    "\n",
    "# ClusteringModel 클래스 인스턴스 생성\n",
    "clustering_model = ClusteringModel(data=coords)\n",
    "kmeans_model = clustering_model.kmeans_clustering(n_clusters=25, \n",
    "                                                train_data=train_data, \n",
    "                                                test_data=test_data, \n",
    "                                                feature_columns=feature_columns, \n",
    "                                                label_column=\"region\")\n",
    "\n",
    "region_mean_prices = train_data.groupby(\"region\")[\"deposit\"].mean().reset_index()\n",
    "region_mean_prices.columns = [\"region\", \"mean_deposit\"]\n",
    "region_mean_prices[\"mean_deposit_category\"] = region_mean_prices[\"mean_deposit\"] // 10000\n",
    "\n",
    "# train_data와 region_mean_prices 병합\n",
    "train_data = train_data.merge(region_mean_prices, on=\"region\", how=\"left\")\n",
    "test_data = test_data.merge(region_mean_prices, on=\"region\", how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Log변환**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = apply_log_transformation(train_data, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature select**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_data, test_data = select_features(train_data, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**train_data split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = split_features_and_target(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['index', 'area_m2', 'contract_year_month', 'contract_day',\n",
       "       'contract_type', 'floor', 'built_year', 'latitude', 'longitude', 'age',\n",
       "       'interest_rate', 'nearest_subway_distance', 'nearest_subway_latitude',\n",
       "       'nearest_subway_longitude', 'nearest_school_distance',\n",
       "       'nearest_school_latitude', 'nearest_school_longitude',\n",
       "       'nearest_elementary_distance', 'nearest_elementary_latitude',\n",
       "       'nearest_elementary_longitude', 'nearest_middle_distance',\n",
       "       'nearest_middle_latitude', 'nearest_middle_longitude',\n",
       "       'nearest_high_distance', 'nearest_high_latitude',\n",
       "       'nearest_high_longitude', 'nearest_park_distance',\n",
       "       'nearest_park_latitude', 'nearest_park_longitude', 'park_area',\n",
       "       'park_area_above_300', 'nearest_subway_num', 'nearest_school_num',\n",
       "       'nearest_park_num', 'num_of_subways_within_radius',\n",
       "       'num_of_schools_within_radius', 'num_of_parks_within_radius', 'region',\n",
       "       'mean_deposit', 'mean_deposit_category', 'log_area_m2',\n",
       "       'log_school_distance', 'log_elementary_distance', 'log_middle_distance',\n",
       "       'log_high_distance', 'log_park_distance', 'log_subway_distance'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['deposit', 'log_deposit'], dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Train and Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**XGB**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_params = {\n",
    "#             \"n_estimators\": 249,\n",
    "#             \"learning_rate\": 0.1647758714498898,\n",
    "#             \"max_depth\": 12,\n",
    "#             \"subsample\": 0.9996749158433582\n",
    "#         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xgb_model = XGBoost(**best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mae = cv_train(xgb_model, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tabnet**\n",
    "- 테이블 데이터에서도 딥러닝이 잘 동작할 수 있게 만들어진 모델\n",
    "- 자동으로 중요한 features를 선택하기 떄문에 feature select부분은 제외"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import mean_absolute_error\n",
    "# from sklearn.model_selection import KFold\n",
    "\n",
    "# def cv_train(model, X: pd.DataFrame, y: pd.DataFrame, verbose: bool = True) -> float:\n",
    "#     \"\"\"K-Fold를 이용하여 Cross Validation을 수행하는 함수입니다.\n",
    "\n",
    "#     Args:\n",
    "#         model: 수행하려는 모델\n",
    "#         X (pd.DataFrame): 독립 변수\n",
    "#         y (pd.DataFrame): 예측 변수. deposit과 log_deposit 열로 나뉨.\n",
    "#         verbose (bool, optional): Fold별 진행상황을 출력할지 여부. Defaults to True.\n",
    "\n",
    "#     Returns:\n",
    "#         float: 평균 MAE\n",
    "#     \"\"\"\n",
    "#     cv = 5\n",
    "#     kfold = KFold(n_splits=cv, shuffle=True, random_state=42)\n",
    "\n",
    "#     mae_list = []\n",
    "#     for i, (train_idx, valid_idx) in enumerate(kfold.split(X, y), start=1):\n",
    "#         if verbose: print(f\"training...[{i}/{cv}]\")\n",
    "\n",
    "#         X_train, y_train = X.loc[train_idx, :].values, y.loc[train_idx, \"log_deposit\"].values.reshape(-1, 1)\n",
    "#         X_valid, y_valid = X.loc[valid_idx, :].values, y.loc[valid_idx, \"deposit\"].values.reshape(-1, 1)\n",
    "\n",
    "#         model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], eval_metric=[\"mae\"])\n",
    "\n",
    "#         y_pred = model.predict(X_valid)\n",
    "#         y_pred = np.expm1(y_pred)\n",
    "#         fold_mae = mean_absolute_error(y_valid, y_pred)\n",
    "#         if verbose: print(f\"Valid MAE: {fold_mae:.4f}\")\n",
    "#         mae_list.append(fold_mae)\n",
    "\n",
    "#     mae = np.mean(mae_list)\n",
    "#     if verbose:\n",
    "#         print(\"### K-fold Result ###\")\n",
    "#         print(f\"Valid MAE: {mae:.4f}\")\n",
    "    \n",
    "#     return mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def objective(trial):\n",
    "#     params = {\n",
    "#         \"n_d\": trial.suggest_int(\"n_d\", 16, 64),\n",
    "#         \"n_steps\": trial.suggest_int(\"n_steps\", 3, 10),\n",
    "#         \"gamma\": trial.suggest_float(\"gamma\", 1.0, 2.0),\n",
    "#         \"lambda_sparse\": trial.suggest_float(\"lambda_sparse\", 0.0001, 0.01),\n",
    "#         \"optimizer_fn\": torch.optim.Adam,\n",
    "#         \"optimizer_params\": dict(lr=trial.suggest_float(\"learning_rate\", 0.01, 0.1)),\n",
    "#     }\n",
    "#     model = TabNetRegressor(**params)\n",
    "#     return cv_train(model, X, y, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import mean_absolute_error\n",
    "# from sklearn.model_selection import KFold, cross_val_predict\n",
    "\n",
    "# def objective(trial):\n",
    "#     params = {\n",
    "#         \"n_d\": trial.suggest_int(\"n_d\", 8, 64),\n",
    "#         \"n_a\": trial.suggest_int(\"n_a\", 8, 64),\n",
    "#         \"n_steps\": trial.suggest_int(\"n_steps\", 3, 10),\n",
    "#         \"gamma\": trial.suggest_float(\"gamma\", 1.0, 2.0),\n",
    "#         \"lambda_sparse\": trial.suggest_float(\"lambda_sparse\", 0.0001, 0.01),\n",
    "#         \"optimizer_params\": dict(lr=trial.suggest_float(\"learning_rate\", 0.01, 0.1)),\n",
    "#     }\n",
    "    \n",
    "#     # K-Fold 교차 검증\n",
    "#     cv = 5\n",
    "#     kfold = KFold(n_splits=cv, shuffle=True, random_state=42)\n",
    "#     mae_list = []\n",
    "    \n",
    "#     for i, (train_idx, valid_idx) in enumerate(kfold.split(X, y), start=1):\n",
    "#         if True: print(f\"training...[{i}/{cv}]\")\n",
    "#         X_train, y_train = X.loc[train_idx, :].values, y.loc[train_idx, \"log_deposit\"].values.reshape(-1, 1)\n",
    "#         X_valid, y_valid = X.loc[valid_idx, :].values, y.loc[valid_idx, \"deposit\"].values.reshape(-1, 1)\n",
    "        \n",
    "#         model = TabNetRegressor(**params)\n",
    "#         # 모델 학습 (patience : 성능 개선되지않으면 early stopping)\n",
    "#         model.fit(\n",
    "#             X_train, y_train, \n",
    "#             eval_set=[(X_valid, y_valid)], \n",
    "#             eval_metric=[\"mae\"], \n",
    "#             max_epochs=100,\n",
    "#             patience=10,\n",
    "#             batch_size=1024,\n",
    "#             virtual_batch_size=128,\n",
    "#         )\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "#         # 검증 데이터에 대한 예측\n",
    "#         y_pred = model.predict(X_valid.values)\n",
    "#         y_pred = np.expm1(y_pred)  # 로그 변환의 역변환\n",
    "        \n",
    "#         # MAE 계산\n",
    "#         mae = mean_absolute_error(y_valid, y_pred) \n",
    "#         mae_list.append(mae)\n",
    "\n",
    "#     # 교차 검증 후 MAE 평균값 반환\n",
    "#     return np.mean(mae_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import KFold, cross_val_predict\n",
    "\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        \"n_d\": trial.suggest_int(\"n_d\", 8, 64),\n",
    "        \"n_a\": trial.suggest_int(\"n_a\", 8, 64),\n",
    "        \"n_steps\": trial.suggest_int(\"n_steps\", 3, 10),\n",
    "        \"gamma\": trial.suggest_float(\"gamma\", 1.0, 2.0),\n",
    "        \"lambda_sparse\": trial.suggest_float(\"lambda_sparse\", 0.0001, 0.01),\n",
    "        \"optimizer_params\": dict(lr=trial.suggest_float(\"learning_rate\", 0.001, 0.01))\n",
    "    }\n",
    "    \n",
    "    # K-Fold 교차 검증\n",
    "    cv = 5\n",
    "    kfold = KFold(n_splits=cv, shuffle=True, random_state=42)\n",
    "    \n",
    "    model = TabNetRegressor(**params)\n",
    "    y_pred = cross_val_predict(\n",
    "        model,\n",
    "        X.values, \n",
    "        y[\"log_deposit\"].values.reshape(-1, 1),\n",
    "        cv = kfold,\n",
    "        method=\"predict\",\n",
    "        fit_params={\"max_epochs\": 5}\n",
    "    )\n",
    "    \n",
    "    y_pred = np.expm1(y_pred)\n",
    "        \n",
    "    # MAE 계산\n",
    "    mae = mean_absolute_error(np.expm1(y[\"log_deposit\"].values), y_pred) \n",
    "    print(f\"Trial {trial.number}: MAE = {mae}\")\n",
    "\n",
    "    # 교차 검증 후 MAE 반환\n",
    "    return mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-21 20:49:46,729] A new study created in memory with name: no-name-11e6efd9-6e41-40f2-b066-3a1016b746bb\n",
      "/opt/conda/lib/python3.10/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:73: FutureWarning: `fit_params` is deprecated and will be removed in version 1.6. Pass parameters via `params` instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.69008 |  0:02:14s\n",
      "epoch 1  | loss: 0.12369 |  0:04:28s\n",
      "epoch 2  | loss: 0.11281 |  0:06:43s\n",
      "epoch 3  | loss: 0.10351 |  0:08:56s\n",
      "epoch 4  | loss: 0.10248 |  0:11:13s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/opt/conda/lib/python3.10/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.73316 |  0:02:08s\n",
      "epoch 1  | loss: 0.11926 |  0:04:18s\n",
      "epoch 2  | loss: 0.10545 |  0:06:26s\n",
      "epoch 3  | loss: 0.10095 |  0:08:35s\n",
      "epoch 4  | loss: 0.09378 |  0:10:44s\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = study.best_params\n",
    "print(\"Best parameters for Tabnet: \", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = TabNetRegressor(**best_params)\n",
    "best_model.fit(X.values, y[\"log_deposit\"].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.metrics import mean_absolute_error\n",
    "# from sklearn.model_selection import KFold\n",
    "# from model.TreeModel import XGBoost, LightGBM, CatBoost\n",
    "# import optuna\n",
    "# RANDOM_SEED = 42\n",
    "\n",
    "# def set_model(model_name: str, **params):\n",
    "#     \"\"\"주어진 모델 이름에 따라 모델을 생성하고 반환하는 함수입니다.\n",
    "\n",
    "#     Args:\n",
    "#         model_name (str): 생성하려는 모델 이름\n",
    "#         **params (dict): 모델 생성 시 사용할 하이퍼파라미터\n",
    "\n",
    "#     Returns:\n",
    "#         model (object): 생성된 모델 객체\n",
    "#     \"\"\"\n",
    "#     match model_name:\n",
    "#         case \"xgboost\":\n",
    "#             model = XGBoost(**params)\n",
    "#         case \"lightgbm\":\n",
    "#             model = LightGBM(**params)\n",
    "#         case \"catboost\":\n",
    "#             model = CatBoost(**params)\n",
    "#         case \"tabnet\":\n",
    "#             model = TabNetRegressor(**params)\n",
    "#     return model\n",
    "\n",
    "# def cv_train(model, X: pd.DataFrame, y: pd.DataFrame, verbose: bool = True) -> float:\n",
    "#     \"\"\"K-Fold를 이용하여 Cross Validation을 수행하는 함수입니다.\n",
    "\n",
    "#     Args:\n",
    "#         model: 수행하려는 모델\n",
    "#         X (pd.DataFrame): 독립 변수\n",
    "#         y (pd.DataFrame): 예측 변수. deposit과 log_deposit 열로 나뉨.\n",
    "#         verbose (bool, optional): Fold별 진행상황을 출력할지 여부. Defaults to True.\n",
    "\n",
    "#     Returns:\n",
    "#         float: 평균 MAE\n",
    "#     \"\"\"\n",
    "#     cv = 5\n",
    "#     kfold = KFold(n_splits=cv, shuffle=True, random_state=42)\n",
    "\n",
    "#     mae_list = []\n",
    "#     for i, (train_idx, valid_idx) in enumerate(kfold.split(X, y), start=1):\n",
    "#         if verbose: print(f\"training...[{i}/{cv}]\")\n",
    "\n",
    "#         X_train, y_train = X.loc[train_idx, :].values, y.loc[train_idx, \"log_deposit\"].values.reshape(-1,1)\n",
    "#         X_valid, y_valid = X.loc[valid_idx, :].values, y.loc[valid_idx, \"deposit\"].values.reshape(-1,1)\n",
    "\n",
    "#         model.fit(\n",
    "#             X_train, y_train, \n",
    "#             eval_set=[(X_valid, y_valid)], \n",
    "#             eval_metric=[\"mae\"], \n",
    "#             max_epochs=100,\n",
    "#             patience=10\n",
    "#         )\n",
    "\n",
    "#         y_pred = model.predict(X_valid)\n",
    "#         y_pred = np.expm1(y_pred)\n",
    "#         fold_mae = mean_absolute_error(y_valid, y_pred)\n",
    "#         if verbose: print(f\"Valid MAE: {fold_mae:.4f}\")\n",
    "#         mae_list.append(fold_mae)\n",
    "\n",
    "#     mae = np.mean(mae_list)\n",
    "#     if verbose:\n",
    "#         print(\"### K-fold Result ###\")\n",
    "#         print(f\"Valid MAE: {mae:.4f}\")\n",
    "    \n",
    "#     return mae\n",
    "\n",
    "# def optuna_train(model_name: str, X: pd.DataFrame, y: pd.DataFrame) -> tuple[dict, float]:\n",
    "#     \"\"\"Optuna를 사용하여 주어진 모델의 하이퍼파라미터를 최적하는 함수\n",
    "\n",
    "#     Args:\n",
    "#         model_name (str): 최적화할 모델의 이름\n",
    "#         X (pd.DataFrame): 독립 변수\n",
    "#         y (pd.DataFrame): 예측 변수\n",
    "\n",
    "#     Returns:\n",
    "#         tuple[dict, float]:\n",
    "#             - dict: 최적의 하이퍼파라미터\n",
    "#             - float: 최적의 하이퍼파라미터에 대한 성능 지표(MAE)\n",
    "#     \"\"\"\n",
    "#     def objective(trial):\n",
    "#         match model_name:\n",
    "#             case \"xgboost\":\n",
    "#                 params = {\n",
    "#                     \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 300),\n",
    "#                     \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.2),\n",
    "#                     \"max_depth\": trial.suggest_int(\"max_depth\", 5, 12),\n",
    "#                     \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "#                 }\n",
    "#             case \"lightgbm\":\n",
    "#                 params = {\n",
    "#                     \"verbose\": -1,\n",
    "#                     \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 300),\n",
    "#                     \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True),\n",
    "#                     \"max_depth\": trial.suggest_int(\"max_depth\", 5, 12),\n",
    "#                     \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "#                     \"num_leaves\": trial.suggest_int(\"num_leaves\", 20, 150),\n",
    "#                     \"objective\": \"regression_l1\"\n",
    "#             }\n",
    "#             case \"catboost\":\n",
    "#                 params = {\n",
    "#                     \"verbose\": 0,\n",
    "#                     \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n",
    "#                     \"iterations\": trial.suggest_int(\"iterations\", 50, 500),\n",
    "#                     \"depth\": trial.suggest_int(\"depth\", 3, 10),\n",
    "#                     \"l2_leaf_reg\": trial.suggest_int(\"l2_leaf_reg\", 1, 10),\n",
    "#                     # \"bagging_temperature\": trial.suggest_loguniform(\"bagging_temperature\", 0.01, 1),\n",
    "#                     # \"border_count\": trial.suggest_int(\"border_count\", 32, 255),\n",
    "#                     \"cat_features\": [\"contract_day\"],\n",
    "#                     \"task_type\": \"GPU\",\n",
    "#                     \"devices\": \"cuda\",\n",
    "#                 }\n",
    "#             case \"tabnet\":\n",
    "#                 params = {\n",
    "#                     \"n_d\": trial.suggest_int(\"n_d\", 8, 64),\n",
    "#                     \"n_steps\": trial.suggest_int(\"n_steps\", 3, 10),\n",
    "#                     \"gamma\": trial.suggest_float(\"gamma\", 1.0, 2.0),\n",
    "#                     \"lambda_sparse\": trial.suggest_float(\"lambda_sparse\", 0.0001, 0.01),\n",
    "#                     \"optimizer_params\": dict(lr=trial.suggest_float(\"learning_rate\", 0.01, 0.1)),\n",
    "#                 }\n",
    "#         model = set_model(model_name, **params)\n",
    "#         return cv_train(model, X, y, verbose=False)\n",
    "    \n",
    "#     sampler = optuna.samplers.TPESampler(seed=42)\n",
    "#     study = optuna.create_study(direction=\"minimize\", sampler=sampler)\n",
    "#     study.optimize(objective, n_trials=50)\n",
    "#     return study.best_params, study.best_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_params, mae = optuna_train(\"tabnet\", X, y)\n",
    "# best_model = set_model(\"tabnet\", **best_params)\n",
    "# best_model = best_model.fit(X.values, y[\"log_deposit\"].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_csv(best_model, test_data, sample_submission)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

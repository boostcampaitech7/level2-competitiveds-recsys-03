{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 라이브러리 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from data.load_dataset import load_dataset\n",
    "from data.merge_dataset import merge_dataset\n",
    "from data.feature_engineering import apply_log_transformation, ClusteringModel\n",
    "# from model.inference import save_csv\n",
    "# from model.feature_select import select_features\n",
    "# from model.data_split import split_features_and_target\n",
    "# from model.model_train import cv_train, set_model, optuna_train\n",
    "import argparse\n",
    "import os\n",
    "import wandb\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기존 데이터 불러오기\n",
    "train_data, test_data, sample_submission, interest_data, subway_data, school_data, park_data = load_dataset()\n",
    "\n",
    "# 기존 데이터에 새로운 feature들을 병합한 데이터프레임 불러오기\n",
    "train_data, test_data = merge_dataset(train_data, test_data, interest_data, subway_data, school_data, park_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위치 중복도 낮은 행 삭제\n",
    "groups = train_data.groupby([\"latitude\", \"longitude\"])[\"index\"].count()\n",
    "conditioned_groups_index = groups[(groups >= 2) & (groups <= 5)].index # 이 범위를 파라미터로 조정하는걸로\n",
    "small_groups = train_data[\n",
    "    train_data[\"latitude\"].isin(conditioned_groups_index.get_level_values(0)) &\n",
    "    train_data[\"longitude\"].isin(conditioned_groups_index.get_level_values(1))\n",
    "]\n",
    "train_data.drop(small_groups.index, axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# built_year > 2024 행 삭제\n",
    "train_data = train_data[train_data[\"built_year\"] < 2024]\n",
    "train_data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log 변환\n",
    "train_data, test_data = apply_log_transformation(train_data, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 가격 Clustering EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 먼저 데이터 정렬 및 인덱스 리셋\n",
    "sorted_train_data = train_data.sort_values(by=\"deposit\").reset_index(drop=True)\n",
    "# deposit을 기준으로 그룹을 나눔\n",
    "# 10,000 미만은 그룹 0, 10,000~100,000 사이는 그룹 1, 100,000 이상은 100,000 단위로 그룹화\n",
    "def categorize_deposit(deposit):\n",
    "    if deposit < 10000:\n",
    "        return 0  # 10,000 미만\n",
    "    elif deposit <= 100000:\n",
    "        return 1  # 10,000 ~ 100,000 사이\n",
    "    elif deposit <= 200000:\n",
    "        return 2\n",
    "    elif deposit <= 300000:\n",
    "        return 3\n",
    "    elif deposit <= 400000:\n",
    "        return 4\n",
    "    elif deposit <= 500000:\n",
    "        return 5\n",
    "    elif deposit <= 600000:\n",
    "        return 6\n",
    "    else:\n",
    "        return 7 \n",
    "# 그룹화 적용\n",
    "sorted_train_data[\"deposit_group\"] = sorted_train_data[\"deposit\"].apply(categorize_deposit)\n",
    "# 그룹별 통계 출력\n",
    "train_data = sorted_train_data\n",
    "print(train_data.groupby('deposit_group')['deposit'].agg(['min', 'max', 'mean', 'count']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Select\n",
    "selected_cols = [\"log_area_m2\",\"built_year\",\"latitude\",\"longitude\",\"log_leader_distance\",\"log_subway_distance\",\"contract_year_month\",\"num_of_subways_within_radius\",\"park_exists\",\"region\",\"region_mean\"]\n",
    "X, test_data = train_data[selected_cols], test_data[selected_cols]\n",
    "\n",
    "# Data Split\n",
    "y = train_data[\"deposit_group\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"X: {X.columns}\")\n",
    "print(f\"test_data: {test_data.columns}\")\n",
    "print(f\"y: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import log_loss, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import optuna\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = str(input(\"run 이름을 입력하세요 :\"))\n",
    "selected_model = str(input(\"model 명을 입력하세요 (xgb/rf) :\"))\n",
    "opt = bool(input(\"Optuna 사용 여부를 입력하세요 (뭐라도 입력 시 사용) :\"))\n",
    "\n",
    "wandb.init(\n",
    "    settings=wandb.Settings(start_method=\"thread\"),\n",
    "    dir=None,  # 로컬에 로그 저장하지 않음\n",
    "    entity=\"remember-us\", # team name,\n",
    "    project=\"deposit\", # project name\n",
    "    name=run, # run name\n",
    "    config={\n",
    "        \"User\": os.path.basename(os.path.dirname(os.getcwd())) # jupyter는 이렇게\n",
    "    } # common setting\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model_name, X, y, params):\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    match model_name:\n",
    "        case \"xgb-cls\":\n",
    "            model = xgb.XGBClassifier(**params, random_state=42, device=\"cuda\", use_label_encoder=False, n_jobs=-1)\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "            y_pred_proba = model.predict_proba(X_valid)\n",
    "            score = log_loss(y_valid, y_pred_proba)\n",
    "            \n",
    "        case \"xgb-reg\":\n",
    "            model = xgb.XGBRegressor(**params, random_state=42, device=\"cuda\", n_jobs=-1)\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "            y_pred = model.predict(X_valid)\n",
    "            score = mean_absolute_error(y_valid, y_pred)\n",
    "        \n",
    "        case \"rf-cls\":\n",
    "            model = RandomForestClassifier(**params, random_state=42, n_jobs=-1)\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "            y_pred_proba = model.predict_proba(X_valid)\n",
    "            score = log_loss(y_valid, y_pred_proba)\n",
    "        \n",
    "        case \"rf-reg\":\n",
    "            model = RandomForestRegressor(**params, random_state=42, n_jobs=-1)\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "            y_pred = model.predict(X_valid)\n",
    "            score = mean_absolute_error(y_valid, y_pred)\n",
    "            \n",
    "        case _:\n",
    "            raise ValueError(f\"지원하지 않는 모델 이름: {model_name}\")\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_train(model_name, X, y, params):\n",
    "    cv = 5\n",
    "    kfold = KFold(n_splits=cv, shuffle=True, random_state=42)\n",
    "\n",
    "    score_list = []\n",
    "    for i, (train_idx, valid_idx) in enumerate(kfold.split(X, y), start=1):\n",
    "        X_train, y_train = X.loc[train_idx, :], y.iloc[train_idx]\n",
    "        X_valid, y_valid = X.loc[valid_idx, :], y.iloc[valid_idx]\n",
    "        \n",
    "        match model_name:\n",
    "            case \"xgb-cls\":\n",
    "                model = xgb.XGBClassifier(**params, random_state=42, device=\"cuda\", use_label_encoder=False, n_jobs=-1)\n",
    "                model.fit(X_train, y_train)\n",
    "\n",
    "                y_pred_proba = model.predict_proba(X_valid)\n",
    "                fold_score = log_loss(y_valid, y_pred_proba)\n",
    "                \n",
    "            case \"xgb-reg\":\n",
    "                model = xgb.XGBRegressor(**params, random_state=42, device=\"cuda\", n_jobs=-1)\n",
    "                model.fit(X_train, y_train)\n",
    "\n",
    "                y_pred = model.predict(X_valid)\n",
    "                fold_score = mean_absolute_error(y_valid, y_pred)\n",
    "            \n",
    "            case \"rf-cls\":\n",
    "                model = RandomForestClassifier(**params, random_state=42, n_jobs=-1)\n",
    "                model.fit(X_train, y_train)\n",
    "\n",
    "                y_pred_proba = model.predict_proba(X_valid)\n",
    "                fold_score = log_loss(y_valid, y_pred_proba)\n",
    "            \n",
    "            case \"rf-reg\":\n",
    "                model = RandomForestRegressor(**params, random_state=42, n_jobs=-1)\n",
    "                model.fit(X_train, y_train)\n",
    "\n",
    "                y_pred = model.predict(X_valid)\n",
    "                fold_score = mean_absolute_error(y_valid, y_pred)\n",
    "                \n",
    "            case _:\n",
    "                raise ValueError(f\"지원하지 않는 모델 이름: {model_name}\")\n",
    "                \n",
    "        score_list.append(fold_score)\n",
    "    mean_score = np.mean(score_list)\n",
    "    return mean_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optuna_train(model_name, X, y):\n",
    "    match model_name:\n",
    "        case \"xgb-cls\":\n",
    "            def objective(trial):\n",
    "                params = {\n",
    "                    \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 300),\n",
    "                    \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.2),\n",
    "                    \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "                    \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "                    \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "                    \"gamma\": trial.suggest_float(\"gamma\", 0, 5),\n",
    "                }\n",
    "\n",
    "                return train(model_name, X, y, params)\n",
    "\n",
    "        case \"xgb-reg\":\n",
    "            def objective(trial):\n",
    "                params = {\n",
    "                    \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 300),\n",
    "                    \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.2),\n",
    "                    \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "                    \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "                    \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "                    \"gamma\": trial.suggest_float(\"gamma\", 0, 5),\n",
    "                }\n",
    "                \n",
    "                return train(model_name, X, y, params)\n",
    "         \n",
    "        case \"rf-cls\":\n",
    "            def objective(trial):\n",
    "                params = {\n",
    "                    \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 300),\n",
    "                    \"max_depth\": trial.suggest_int(\"max_depth\", 1, 30),  # 깊이를 1에서 30으로 조정\n",
    "                    \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 10),  # 최소 샘플 분할 수\n",
    "                    \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 1, 10),  # 최소 리프 샘플 수\n",
    "                }\n",
    "\n",
    "                return train(model_name, X, y, params)\n",
    "        \n",
    "        case \"rf-reg\":\n",
    "            def objective(trial):\n",
    "                params = {\n",
    "                    \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 300),\n",
    "                    \"max_depth\": trial.suggest_int(\"max_depth\", 1, 30),  # 깊이를 1에서 30으로 조정\n",
    "                    \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 10),  # 최소 샘플 분할 수\n",
    "                    \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 1, 10),  # 최소 리프 샘플 수\n",
    "                }\n",
    "\n",
    "                return train(model_name, X, y, params)\n",
    "\n",
    "        case _:\n",
    "            raise ValueError(f\"지원하지 않는 모델 이름: {model_name}\")\n",
    "    \n",
    "    sampler = optuna.samplers.TPESampler(seed=42)\n",
    "    study = optuna.create_study(direction=\"minimize\", sampler=sampler)\n",
    "    study.optimize(objective, n_trials=50)\n",
    "    return study.best_params, study.best_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### deposit_group 학습 및 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "match selected_model:\n",
    "    case \"xgb\": model_name = \"xgb-cls\"\n",
    "    case \"rf\": model_name = \"rf-cls\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if opt:\n",
    "    best_params, _ = optuna_train(model_name, X, y)\n",
    "else:\n",
    "    best_params = {'n_estimators': 269, 'max_depth': 24, 'min_samples_split': 2, 'min_samples_leaf': 3}\n",
    "\n",
    "# valid_logloss = cv_train(model_name, X, y, best_params)\n",
    "# print(f\"Valid logloss = {valid_logloss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match selected_model:\n",
    "    case \"xgb\":\n",
    "        best_model = xgb.XGBClassifier(**best_params, random_state=42, device=\"cuda\", use_label_encoder=False, n_jobs=-1)\n",
    "    case \"rf\":\n",
    "        best_model = RandomForestClassifier(**best_params, random_state=42, n_jobs=-1)\n",
    "\n",
    "best_model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_groups = best_model.predict(test_data)\n",
    "test_data['predicted_group'] = predicted_groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### deposit 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "match selected_model:\n",
    "    case \"xgb\": model_name = \"xgb-reg\"\n",
    "    case \"rf\": model_name = \"rf-reg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_params = {\n",
    "        0: {'n_estimators': 299,\n",
    "        'learning_rate': 0.07528019634863661,\n",
    "        'max_depth': 12,\n",
    "        'subsample': 0.9826861644413183,\n",
    "        'colsample_bytree': 0.6361974955396621,\n",
    "        'gamma': 0.13938848002312465},\n",
    "    1: {'n_estimators': 276,\n",
    "        'learning_rate': 0.15579191199373718,\n",
    "        'max_depth': 12,\n",
    "        'subsample': 0.909150931054429,\n",
    "        'colsample_bytree': 0.8709809907337003,\n",
    "        'gamma': 3.936332525239126},\n",
    "    2: {'n_estimators': 282,\n",
    "        'learning_rate': 0.05353956138863308,\n",
    "        'max_depth': 12,\n",
    "        'subsample': 0.9346361282442123,\n",
    "        'colsample_bytree': 0.9739714687453176,\n",
    "        'gamma': 4.082676340052684},\n",
    "    3: {'n_estimators': 164,\n",
    "        'learning_rate': 0.08351992311654627,\n",
    "        'max_depth': 12,\n",
    "        'subsample': 0.9598482300597749,\n",
    "        'colsample_bytree': 0.8213376377326619,\n",
    "        'gamma': 0.13938848002312465},\n",
    "    4: {'n_estimators': 128,\n",
    "        'learning_rate': 0.1224960449028662,\n",
    "        'max_depth': 8,\n",
    "        'subsample': 0.882895691571187,\n",
    "        'colsample_bytree': 0.7384358080293545,\n",
    "        'gamma': 1.3643518616482488},\n",
    "    5: {'n_estimators': 60,\n",
    "        'learning_rate': 0.0195312280618686,\n",
    "        'max_depth': 8,\n",
    "        'subsample': 0.604197860374508,\n",
    "        'colsample_bytree': 0.9606755064214969,\n",
    "        'gamma': 3.8599892180273407},\n",
    "    6: {'n_estimators': 104,\n",
    "        'learning_rate': 0.1408633637803759,\n",
    "        'max_depth': 7,\n",
    "        'subsample': 0.763889446515016,\n",
    "        'colsample_bytree': 0.8858320716284691,\n",
    "        'gamma': 0.9091078632943758},\n",
    "    7: {'n_estimators': 172,\n",
    "        'learning_rate': 0.164450647273469,\n",
    "        'max_depth': 4,\n",
    "        'subsample': 0.7642767983066922,\n",
    "        'colsample_bytree': 0.8095967005933605,\n",
    "        'gamma': 0.8290811474680035}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_regressors_per_group(train_data):\n",
    "    group_models = {}\n",
    "    group_params = {}\n",
    "    group_scores = {}\n",
    "    for group in tqdm(train_data['deposit_group'].unique(), desc=\"Training models per group\"):\n",
    "        group_data = train_data[train_data['deposit_group'] == group]\n",
    "        X_group = group_data[selected_cols]\n",
    "        y_group = group_data['deposit']\n",
    "\n",
    "        # 모델 훈련\n",
    "        if opt:\n",
    "            best_params, best_value = optuna_train(model_name, X_group, y_group) # 26m\n",
    "            group_params[group] = best_params\n",
    "            group_scores[group] = best_value\n",
    "        else:\n",
    "            best_params = group_params[group]\n",
    "            score = train(model_name, X_group, y_group, best_params)\n",
    "            group_scores[group] = score\n",
    "        \n",
    "        match selected_model:\n",
    "            case \"xgb\": model = xgb.XGBRegressor(**best_params, random_state=42, device=\"cuda\", n_jobs=-1)\n",
    "            case \"rf\": model = RandomForestRegressor(**best_params, random_state=42, n_jobs=-1)\n",
    "        model.fit(X_group, y_group)\n",
    "        \n",
    "        # 각 그룹에 해당하는 모델 저장\n",
    "        group_models[group] = model\n",
    "        \n",
    "    return group_models, group_params, group_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. `deposit_group` 별로 회귀 모델 훈련\n",
    "group_models, group_params, group_scores = train_regressors_per_group(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = train_data.groupby('deposit_group')['deposit'].count()\n",
    "scores = sum(score * counts[group] for group, score in group_scores.items())\n",
    "total_count = counts.sum()\n",
    "mean_score = scores / total_count\n",
    "\n",
    "print(f\"Mean MAE: {mean_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_per_group(test_data, group_models):\n",
    "    # 예측값을 저장할 배열 초기화\n",
    "    y_pred = np.zeros(len(test_data))\n",
    "    \n",
    "    # 그룹별로 데이터 분리 후 예측\n",
    "    for group, model in group_models.items():\n",
    "        group_data = test_data[test_data['predicted_group'] == group]\n",
    "        X_group = group_data[selected_cols]\n",
    "        \n",
    "        # 각 그룹에 대해 예측\n",
    "        if len(X_group) > 0:  # 해당 그룹에 데이터가 있는 경우만 예측\n",
    "            y_pred_group = model.predict(X_group)\n",
    "            y_pred[test_data['predicted_group'] == group] = y_pred_group\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = predict_per_group(test_data, group_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_params = {int(k): v for k, v in group_params.items()}\n",
    "wandb.log({\n",
    "    \"features\": selected_cols,\n",
    "    \"model\": selected_model,\n",
    "    \"params\": best_params,\n",
    "    \"group_params\": group_params,\n",
    "    # \"Valid logloss\": valid_logloss,\n",
    "    \"Valid MAE\": mean_score,\n",
    "    \"Optuna\": opt\n",
    "})\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission[\"deposit\"] = y_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = run + \".csv\"\n",
    "sample_submission.to_csv(file_name, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

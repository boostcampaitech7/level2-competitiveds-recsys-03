{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 라이브러리 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from data.load_dataset import load_dataset\n",
    "from data.merge_dataset import merge_dataset\n",
    "from data.feature_engineering import ClusteringModel\n",
    "# from model.inference import save_csv\n",
    "from model.feature_select import select_features\n",
    "from model.data_split import split_features_and_target\n",
    "from model.log_transformation import apply_log_transformation\n",
    "# from model.model_train import cv_train, set_model, optuna_train\n",
    "import argparse\n",
    "import os\n",
    "import wandb\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기존 데이터 불러오기\n",
    "train_data, test_data, sample_submission, interest_data, subway_data, school_data, park_data = load_dataset()\n",
    "\n",
    "# 기존 데이터에 새로운 feature들을 병합한 데이터프레임 불러오기\n",
    "train_data, test_data = merge_dataset(train_data, test_data, interest_data, subway_data, school_data, park_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위치 중복도 낮은 행 삭제\n",
    "groups = train_data.groupby([\"latitude\", \"longitude\"])[\"index\"].count()\n",
    "conditioned_groups_index = groups[(groups >= 2) & (groups <= 5)].index # 이 범위를 파라미터로 조정하는걸로\n",
    "small_groups = train_data[\n",
    "    train_data[\"latitude\"].isin(conditioned_groups_index.get_level_values(0)) &\n",
    "    train_data[\"longitude\"].isin(conditioned_groups_index.get_level_values(1))\n",
    "]\n",
    "train_data.drop(small_groups.index, axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# built_year > 2024 행 삭제\n",
    "train_data = train_data[train_data[\"built_year\"] < 2024]\n",
    "train_data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clustering\n",
    "cluster_data = train_data[[\"latitude\", \"longitude\"]]\n",
    "clustering_model = ClusteringModel(cluster_data)\n",
    "kmeans_model = clustering_model.kmeans_clustering(\n",
    "    n_clusters = 25,\n",
    "    train_data = train_data,\n",
    "    test_data = test_data,\n",
    "    feature_columns = [\"latitude\", \"longitude\"],\n",
    "    label_column = 'region'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# region_mean 병합\n",
    "region_mean = train_data.groupby('region')['deposit'].mean().reset_index()\n",
    "region_mean.columns = ['region', 'region_mean']\n",
    "train_data = train_data.merge(region_mean, on='region', how='left')\n",
    "test_data = test_data.merge(region_mean, on='region', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log 변환\n",
    "train_data, test_data = apply_log_transformation(train_data, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 가격 Clustering EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted_train_data = train_data.sort_values(by=\"deposit\").reset_index(drop=True)\n",
    "# sorted_train_data[\"deposit_group\"] = sorted_train_data.index // 180000\n",
    "# train_data = sorted_train_data\n",
    "# print(train_data.groupby('deposit_group')['deposit'].agg(['min', 'max', 'mean', 'count']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 먼저 데이터 정렬 및 인덱스 리셋\n",
    "sorted_train_data = train_data.sort_values(by=\"deposit\").reset_index(drop=True)\n",
    "# deposit을 기준으로 그룹을 나눔\n",
    "# 10,000 미만은 그룹 0, 10,000~100,000 사이는 그룹 1, 100,000 이상은 100,000 단위로 그룹화\n",
    "def categorize_deposit(deposit):\n",
    "    if deposit < 10000:\n",
    "        return 0  # 10,000 미만\n",
    "    elif deposit <= 100000:\n",
    "        return 1  # 10,000 ~ 100,000 사이\n",
    "    elif deposit <= 200000:\n",
    "        return 2\n",
    "    elif deposit <= 300000:\n",
    "        return 3\n",
    "    elif deposit <= 400000:\n",
    "        return 4\n",
    "    elif deposit <= 500000:\n",
    "        return 5\n",
    "    elif deposit <= 600000:\n",
    "        return 6\n",
    "    else:\n",
    "        return 7 \n",
    "# 그룹화 적용\n",
    "sorted_train_data[\"deposit_group\"] = sorted_train_data[\"deposit\"].apply(categorize_deposit)\n",
    "# 그룹별 통계 출력\n",
    "train_data = sorted_train_data\n",
    "print(train_data.groupby('deposit_group')['deposit'].agg(['min', 'max', 'mean', 'count']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 성능 평가를 위한 holdout 데이터셋\n",
    "holdout_start = 202307\n",
    "holdout_end = 202312\n",
    "holdout_data = train_data[(train_data[\"contract_year_month\"] >= holdout_start) & (train_data[\"contract_year_month\"] <= holdout_end)]\n",
    "train_data = train_data[~((train_data[\"contract_year_month\"] >= holdout_start) & (train_data[\"contract_year_month\"] <= holdout_end))]\n",
    "\n",
    "holdout_data.reset_index(drop=True, inplace=True)\n",
    "train_data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Select\n",
    "selected_cols = [\n",
    "    \"log_area_m2\",\n",
    "    \"built_year\",\n",
    "    \"latitude\",\n",
    "    \"longitude\",\n",
    "    \"log_subway_distance\",\n",
    "    \"log_school_distance\",\n",
    "    \"log_park_distance\",\n",
    "    \"contract_year_month\",\n",
    "    \"contract_day\"\n",
    "    # \"num_of_subways_within_radius\",\n",
    "    # \"num_of_parks_within_radius\",\n",
    "    # \"region\",\n",
    "    # \"region_mean\",\n",
    "]\n",
    "X, test_data, X_hold = train_data[selected_cols], test_data[selected_cols], holdout_data[selected_cols]\n",
    "\n",
    "# Data Split\n",
    "y, y_hold = train_data[\"deposit_group\"], holdout_data[\"deposit\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_selection import SelectKBest, f_regression, f_classif, RFE\n",
    "\n",
    "# def select_kbest(X, y, target, k=10):\n",
    "#     \"\"\"\n",
    "#     SelectKBest 방법을 사용하여 상위 k개의 특성 선택\n",
    "#     Args:\n",
    "#         X (DataFrame): 독립변수\n",
    "#         y (DataFrame): 종속변수\n",
    "#         target (str): 종속변수 열 중 실제 사용할 target 열 이름\n",
    "#         k (int, optional): 선택할 상위 k개 특성의 수 (Defaults to 10)\n",
    "#     Returns:\n",
    "#         List[str]: 선택된 상위 k개의 특성의 열 이름 리스트\n",
    "#     \"\"\"\n",
    "#     # SelectKBest 적용\n",
    "#     selector = SelectKBest(score_func=f_classif, k=k)\n",
    "#     selector.fit(X, y[target])\n",
    "#     # 선택된 특성의 열 이름 리스트 반환\n",
    "#     selected_cols = X.columns[selector.get_support()].tolist()\n",
    "#     return selected_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected_cols = select_kbest(X, train_data, \"deposit_group\")\n",
    "# selected_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"X: {X.columns}\")\n",
    "print(f\"X_hold: {X_hold.columns}\")\n",
    "print(f\"test_data: {test_data.columns}\")\n",
    "print(f\"y: {y.shape}\")\n",
    "print(f\"y_hold: {y_hold.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import log_loss, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import optuna\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = str(input(\"run 이름을 입력하세요 :\"))\n",
    "selected_model = str(input(\"model 명을 입력하세요 (xgb/rf) :\"))\n",
    "opt = bool(input(\"Optuna 사용 여부를 입력하세요 (뭐라도 입력 시 사용) :\"))\n",
    "\n",
    "wandb.init(\n",
    "    settings=wandb.Settings(start_method=\"thread\"),\n",
    "    dir=None,  # 로컬에 로그 저장하지 않음\n",
    "    entity=\"remember-us\", # team name,\n",
    "    project=\"deposit\", # project name\n",
    "    name=run, # run name\n",
    "    config={\n",
    "        \"User\": os.path.basename(os.path.dirname(os.getcwd())) # jupyter는 이렇게\n",
    "    } # common setting\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model_name, X, y, params):\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    match model_name:\n",
    "        case \"xgb-cls\":\n",
    "            model = xgb.XGBClassifier(**params, random_state=42, device=\"cuda\", use_label_encoder=False, n_jobs=-1)\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "            y_pred_proba = model.predict_proba(X_valid)\n",
    "            score = log_loss(y_valid, y_pred_proba)\n",
    "            \n",
    "        case \"xgb-reg\":\n",
    "            model = xgb.XGBRegressor(**params, random_state=42, device=\"cuda\", n_jobs=-1)\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "            y_pred = model.predict(X_valid)\n",
    "            score = mean_absolute_error(y_valid, y_pred)\n",
    "        \n",
    "        case \"rf-cls\":\n",
    "            model = RandomForestClassifier(**params, random_state=42, n_jobs=-1)\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "            y_pred_proba = model.predict_proba(X_valid)\n",
    "            score = log_loss(y_valid, y_pred_proba)\n",
    "        \n",
    "        case \"rf-reg\":\n",
    "            model = RandomForestRegressor(**params, random_state=42, n_jobs=-1)\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "            y_pred = model.predict(X_valid)\n",
    "            score = mean_absolute_error(y_valid, y_pred)\n",
    "            \n",
    "        case _:\n",
    "            raise ValueError(f\"지원하지 않는 모델 이름: {model_name}\")\n",
    "    \n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_train(model_name, X, y, params):\n",
    "    cv = 5\n",
    "    kfold = KFold(n_splits=cv, shuffle=True, random_state=42)\n",
    "\n",
    "    score_list = []\n",
    "    for i, (train_idx, valid_idx) in enumerate(kfold.split(X, y), start=1):\n",
    "        X_train, y_train = X.loc[train_idx, :], y.iloc[train_idx]\n",
    "        X_valid, y_valid = X.loc[valid_idx, :], y.iloc[valid_idx]\n",
    "        \n",
    "        match model_name:\n",
    "            case \"xgb-cls\":\n",
    "                model = xgb.XGBClassifier(**params, random_state=42, device=\"cuda\", use_label_encoder=False, n_jobs=-1)\n",
    "                model.fit(X_train, y_train)\n",
    "\n",
    "                y_pred_proba = model.predict_proba(X_valid)\n",
    "                fold_score = log_loss(y_valid, y_pred_proba)\n",
    "                \n",
    "            case \"xgb-reg\":\n",
    "                model = xgb.XGBRegressor(**params, random_state=42, device=\"cuda\", n_jobs=-1)\n",
    "                model.fit(X_train, y_train)\n",
    "\n",
    "                y_pred = model.predict(X_valid)\n",
    "                fold_score = mean_absolute_error(y_valid, y_pred)\n",
    "            \n",
    "            case \"rf-cls\":\n",
    "                model = RandomForestClassifier(**params, random_state=42, n_jobs=-1)\n",
    "                model.fit(X_train, y_train)\n",
    "\n",
    "                y_pred_proba = model.predict_proba(X_valid)\n",
    "                fold_score = log_loss(y_valid, y_pred_proba)\n",
    "            \n",
    "            case \"rf-reg\":\n",
    "                model = RandomForestRegressor(**params, random_state=42, n_jobs=-1)\n",
    "                model.fit(X_train, y_train)\n",
    "\n",
    "                y_pred = model.predict(X_valid)\n",
    "                fold_score = mean_absolute_error(y_valid, y_pred)\n",
    "                \n",
    "            case _:\n",
    "                raise ValueError(f\"지원하지 않는 모델 이름: {model_name}\")\n",
    "                \n",
    "        score_list.append(fold_score)\n",
    "    mean_score = np.mean(score_list)\n",
    "    return mean_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optuna_train(model_name, X, y):\n",
    "    match model_name:\n",
    "        case \"xgb-cls\":\n",
    "            def objective(trial):\n",
    "                params = {\n",
    "                    \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 300),\n",
    "                    \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.2),\n",
    "                    \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "                    \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "                    \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "                    \"gamma\": trial.suggest_float(\"gamma\", 0, 5),\n",
    "                }\n",
    "\n",
    "                return train(model_name, X, y, params)\n",
    "\n",
    "        case \"xgb-reg\":\n",
    "            def objective(trial):\n",
    "                params = {\n",
    "                    \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 300),\n",
    "                    \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.2),\n",
    "                    \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "                    \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "                    \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "                    \"gamma\": trial.suggest_float(\"gamma\", 0, 5),\n",
    "                }\n",
    "                \n",
    "                return train(model_name, X, y, params)\n",
    "         \n",
    "        case \"rf-cls\":\n",
    "            def objective(trial):\n",
    "                params = {\n",
    "                    \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 300),\n",
    "                    \"max_depth\": trial.suggest_int(\"max_depth\", 1, 30),  # 깊이를 1에서 30으로 조정\n",
    "                    \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 10),  # 최소 샘플 분할 수\n",
    "                    \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 1, 10),  # 최소 리프 샘플 수\n",
    "                }\n",
    "\n",
    "                return train(model_name, X, y, params)\n",
    "        \n",
    "        case \"rf-reg\":\n",
    "            def objective(trial):\n",
    "                params = {\n",
    "                    \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 300),\n",
    "                    \"max_depth\": trial.suggest_int(\"max_depth\", 1, 30),  # 깊이를 1에서 30으로 조정\n",
    "                    \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 10),  # 최소 샘플 분할 수\n",
    "                    \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 1, 10),  # 최소 리프 샘플 수\n",
    "                }\n",
    "\n",
    "                return train(model_name, X, y, params)\n",
    "\n",
    "        case _:\n",
    "            raise ValueError(f\"지원하지 않는 모델 이름: {model_name}\")\n",
    "    \n",
    "    sampler = optuna.samplers.TPESampler(seed=42)\n",
    "    study = optuna.create_study(direction=\"minimize\", sampler=sampler)\n",
    "    study.optimize(objective, n_trials=50)\n",
    "    return study.best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### deposit_group 학습 및 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "match selected_model:\n",
    "    case \"xgb\": model_name = \"xgb-cls\"\n",
    "    case \"rf\": model_name = \"rf-cls\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = {'n_estimators': 144, 'max_depth': 29, 'min_samples_split': 8, 'min_samples_leaf': 6}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if opt:\n",
    "    best_params = optuna_train(model_name, X, y) # 26m\n",
    "else:\n",
    "    best_params = {\n",
    "        'n_estimators': 294,\n",
    "        'learning_rate': 0.12447572794344873,\n",
    "        'max_depth': 12,\n",
    "        'subsample': 0.6648422529245377,\n",
    "        'colsample_bytree': 0.8403361280374215,\n",
    "        'gamma': 0.021125621227948838\n",
    "    }\n",
    "    \n",
    "valid_logloss = cv_train(model_name, X, y, best_params)\n",
    "print(f\"Valid logloss = {valid_logloss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match selected_model:\n",
    "    case \"xgb\":\n",
    "        best_model = xgb.XGBClassifier(**best_params, random_state=42, device=\"cuda\", use_label_encoder=False, n_jobs=-1)\n",
    "    case \"rf\":\n",
    "        best_model = RandomForestClassifier(**best_params, random_state=42, n_jobs=-1)\n",
    "\n",
    "best_model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_groups = best_model.predict(X_hold)\n",
    "X_hold['predicted_group'] = predicted_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_groups = best_model.predict(test_data)\n",
    "test_data['predicted_group'] = predicted_groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### deposit 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "match selected_model:\n",
    "    case \"xgb\": model_name = \"xgb-reg\"\n",
    "    case \"rf\": model_name = \"rf-reg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_params = {\n",
    "    0: {'n_estimators': 271,\n",
    "        'learning_rate': 0.0891731270740701,\n",
    "        'max_depth': 12,\n",
    "        'subsample': 0.9732533070235716,\n",
    "        'colsample_bytree': 0.8126244533797121,\n",
    "        'gamma': 3.7808956754662457},\n",
    "    1: {'n_estimators': 283,\n",
    "        'learning_rate': 0.09647453018390326,\n",
    "        'max_depth': 12,\n",
    "        'subsample': 0.9737151093633044,\n",
    "        'colsample_bytree': 0.9939494366999071,\n",
    "        'gamma': 3.9455604749354842},\n",
    "    2: {'n_estimators': 282,\n",
    "        'learning_rate': 0.1065334717290639,\n",
    "        'max_depth': 12,\n",
    "        'subsample': 0.9702043530164062,\n",
    "        'colsample_bytree': 0.9789964718750591,\n",
    "        'gamma': 3.8378676531756986},\n",
    "    3: {'n_estimators': 282,\n",
    "        'learning_rate': 0.17336659309722366,\n",
    "        'max_depth': 12,\n",
    "        'subsample': 0.9702043530164062,\n",
    "        'colsample_bytree': 0.9697948745561529,\n",
    "        'gamma': 0.12678299219713374},\n",
    "    4: {'n_estimators': 287,\n",
    "        'learning_rate': 0.09993219107638701,\n",
    "        'max_depth': 12,\n",
    "        'subsample': 0.9722495727394489,\n",
    "        'colsample_bytree': 0.9919371080730972,\n",
    "        'gamma': 3.939212978268534},\n",
    "    5: {'n_estimators': 300,\n",
    "        'learning_rate': 0.11132303462474055,\n",
    "        'max_depth': 12,\n",
    "        'subsample': 0.9737522364550457,\n",
    "        'colsample_bytree': 0.9698805295562679,\n",
    "        'gamma': 4.13663053619915},\n",
    "    6: {'n_estimators': 292,\n",
    "        'learning_rate': 0.1038221683452801,\n",
    "        'max_depth': 12,\n",
    "        'subsample': 0.9870909779118721,\n",
    "        'colsample_bytree': 0.9997869198474675,\n",
    "        'gamma': 3.9253609956556277},\n",
    "    7: {'n_estimators': 284,\n",
    "        'learning_rate': 0.1072259934273102,\n",
    "        'max_depth': 12,\n",
    "        'subsample': 0.9340584828362537,\n",
    "        'colsample_bytree': 0.9719409290683207,\n",
    "        'gamma': 4.634228201489641},\n",
    "    8: {'n_estimators': 292,\n",
    "        'learning_rate': 0.1038221683452801,\n",
    "        'max_depth': 12,\n",
    "        'subsample': 0.9870909779118721,\n",
    "        'colsample_bytree': 0.9997869198474675,\n",
    "        'gamma': 3.9253609956556277},\n",
    "    9: {'n_estimators': 295,\n",
    "        'learning_rate': 0.10456616732278738,\n",
    "        'max_depth': 12,\n",
    "        'subsample': 0.9986735852799455,\n",
    "        'colsample_bytree': 0.9979009436789888,\n",
    "        'gamma': 3.8785918564436708}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_regressors_per_group(train_data):\n",
    "    group_models = {}\n",
    "    group_params = {}\n",
    "    for group in tqdm(train_data['deposit_group'].unique(), desc=\"Training models per group\"):\n",
    "        group_data = train_data[train_data['deposit_group'] == group]\n",
    "        X_group = group_data[selected_cols]\n",
    "        y_group = group_data['deposit']\n",
    "\n",
    "        # 모델 훈련\n",
    "        if opt:\n",
    "            best_params = optuna_train(model_name, X_group, y_group) # 26m\n",
    "            group_params[group] = best_params\n",
    "        else:\n",
    "            best_params = group_params[group]\n",
    "        \n",
    "        match selected_model:\n",
    "            case \"xgb\": model = xgb.XGBRegressor(**best_params, random_state=42, device=\"cuda\", n_jobs=-1)\n",
    "            case \"rf\": model = RandomForestRegressor(**best_params, random_state=42, n_jobs=-1)\n",
    "        model.fit(X_group, y_group)\n",
    "        \n",
    "        # 각 그룹에 해당하는 모델 저장\n",
    "        group_models[group] = model\n",
    "        \n",
    "    return group_models, group_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. `deposit_group` 별로 회귀 모델 훈련\n",
    "group_models, group_params = train_regressors_per_group(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_per_group_hold(X_hold, group_models):\n",
    "    # 예측값을 저장할 배열 초기화\n",
    "    y_pred = np.zeros(len(X_hold))\n",
    "    \n",
    "    # 그룹별로 데이터 분리 후 예측\n",
    "    for group, model in group_models.items():\n",
    "        group_data = X_hold[X_hold['predicted_group'] == group]\n",
    "        X_group = group_data[selected_cols]\n",
    "        \n",
    "        # 각 그룹에 대해 예측\n",
    "        if len(X_group) > 0:  # 해당 그룹에 데이터가 있는 경우만 예측\n",
    "            y_pred_group = model.predict(X_group)\n",
    "            y_pred[X_hold['predicted_group'] == group] = y_pred_group\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_per_group(test_data, group_models):\n",
    "    # 예측값을 저장할 배열 초기화\n",
    "    y_pred = np.zeros(len(test_data))\n",
    "    \n",
    "    # 그룹별로 데이터 분리 후 예측\n",
    "    for group, model in group_models.items():\n",
    "        group_data = test_data[test_data['predicted_group'] == group]\n",
    "        X_group = group_data[selected_cols]\n",
    "        \n",
    "        # 각 그룹에 대해 예측\n",
    "        if len(X_group) > 0:  # 해당 그룹에 데이터가 있는 경우만 예측\n",
    "            y_pred_group = model.predict(X_group)\n",
    "            y_pred[test_data['predicted_group'] == group] = y_pred_group\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hold_pred = predict_per_group_hold(X_hold, group_models)\n",
    "y_test_pred = predict_per_group(test_data, group_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hold_mae = mean_absolute_error(holdout_data[\"deposit\"], y_hold_pred)\n",
    "print(f\"Holdout Mean MAE = {hold_mae.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_params = {int(k): v for k, v in group_params.items()}\n",
    "wandb.log({\n",
    "    \"features\": selected_cols,\n",
    "    \"model\": selected_model,\n",
    "    \"params\": best_params,\n",
    "    \"group_params\": group_params,\n",
    "    \"Valid logloss\": valid_logloss,\n",
    "    \"Holdout MAE\": hold_mae,\n",
    "    \"Optuna\": opt\n",
    "})\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission[\"deposit\"] = y_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission.to_csv(\"output.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

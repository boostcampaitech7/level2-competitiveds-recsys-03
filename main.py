from data.feature_engineering import find_nearest_haversine_distance
from data.load_dataset import load_dataset
from data.merge_dataset import merge_dataset
from model.inference import save_csv
from model.feature_select import select_features
from model.data_split import split_features_and_target
from model.log_transformation import apply_log_transformation
import argparse
import os
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, KFold, cross_val_score
import optuna
import wandb
import xgboost as xgb
from sklearn.metrics import mean_absolute_error
import seaborn as sns
import matplotlib.pyplot as plt
from model.model_train import cv_train, set_model

# ë©”ì¸ ì‹¤í–‰ ì½”ë“œ
if __name__ == "__main__":
    parser = argparse.ArgumentParser(prog = "ìˆ˜ë„ê¶Œ ì•„íŒŒíŠ¸ ì „ì„¸ ì˜ˆì¸¡", description="ì‚¬ìš©ë²• python train.py --model ëª¨ë¸ëª…(ì†Œë¬¸ì)")
    parser.add_argument("--model", type=str, choices=["xgboost", "lightgbm", "catboost", "ensemble"], default="xgboost", help="Select the model to train")
    parser.add_argument("--project", type=str, default="no_name", help="Input the project name")
    parser.add_argument("--run", type=str, default="no_name", help="Input the run name")
    args = parser.parse_args()

    # 0. WandB ì´ˆê¸°í™”
    wandb.init(
        settings=wandb.Settings(start_method="thread"),
        dir=None,  # ë¡œì»¬ì— ë¡œê·¸ ì €ì¥í•˜ì§€ ì•ŠìŒ
        entity="remember-us", # team name,
        project=args.project, # project name
        name=args.run, # run name
        config={
            "random_state": 42,
            "device": "cuda"
        } # common setting
    )

    # 1. ë°ì´í„° ë¡œë“œ
    # ê¸°ì¡´ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
    train_data, test_data, sample_submission, interest_data, subway_data, school_data, park_data = load_dataset()
    # ê¸°ì¡´ ë°ì´í„°ì— ìƒˆë¡œìš´ featureë“¤ì„ ë³‘í•©í•œ ë°ì´í„°í”„ë ˆì„ ë¶ˆëŸ¬ì˜¤ê¸°
    train_data, test_data = merge_dataset(train_data, test_data, interest_data, subway_data, school_data, park_data)
    
    # 2. ë°ì´í„° ì „ì²˜ë¦¬
    # ìœ„ì¹˜ ì¤‘ë³µë„ ë‚®ì€ í–‰ ì‚­ì œ
    groups = train_data.groupby(["latitude", "longitude"])["index"].count()
    conditioned_groups_index = groups[(groups >= 2) & (groups <= 5)].index # ì´ ë²”ìœ„ë¥¼ íŒŒë¼ë¯¸í„°ë¡œ ì¡°ì •í•˜ëŠ”ê±¸ë¡œ
    small_groups = train_data[
        train_data["latitude"].isin(conditioned_groups_index.get_level_values(0)) &
        train_data["longitude"].isin(conditioned_groups_index.get_level_values(1))
    ]
    train_data.drop(small_groups.index, axis=0, inplace=True)
    
    # built_year > 2024 í–‰ ì‚­ì œ
    train_data = train_data[train_data["built_year"] < 2024]
    train_data.reset_index(drop=True, inplace=True)
    
    # log ë³€í™˜
    train_data, test_data = apply_log_transformation(train_data, test_data)
    
    # Feature Select
    train_data, test_data, train_cols = select_features(train_data, test_data)
    
    # train_data split
    X, y = split_features_and_target(train_data)
    
    # Model train and evaulate
    best_params = {'n_estimators': 249, 'learning_rate': 0.1647758714498898, 'max_depth': 12, 'subsample': 0.9996749158433582}
    best_model = set_model(args.model, train_type="regression", **best_params)
    mae = cv_train(best_model, X, y)

    best_model = best_model.train(X, y["log_deposit"])
    
    # WandB log and finish
    wandb.log({
        "features": train_cols,
        "model": args.model,
        "params": best_params,
        "valid MAE": mae
    })
    wandb.finish()
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ëŒ€í•œ ì˜ˆì¸¡ ë° ì œì¶œ íŒŒì¼ ìƒì„±
    save_csv(best_model, test_data, sample_submission)

    print(f"ğŸ§¼ [{args.project} - {args.run}] Completed ğŸ§¼")
    
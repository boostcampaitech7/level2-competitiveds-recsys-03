{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Library Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from data.load_dataset import load_dataset\n",
    "from data.merge_dataset import merge_dataset\n",
    "from data.data_preprocessing import *\n",
    "from data.feature_engineering import *\n",
    "from model.inference import save_csv\n",
    "from model.feature_select import select_features\n",
    "from model.data_split import split_features_and_target\n",
    "from model.model_train import set_model, optuna_train\n",
    "#from model.TreeModel import XGBoost\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import optuna\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê¸°ì¡´ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "train_data, test_data, sample_submission, interest_data, subway_data, school_data, park_data = load_dataset()\n",
    "# ê¸°ì¡´ ë°ì´í„°ì— ìƒˆë¡œìš´ featureë“¤ì„ ë³‘í•©í•œ ë°ì´í„°í”„ë ˆì„ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "train_data, test_data = merge_dataset(train_data, test_data, interest_data, subway_data, school_data, park_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìœ„ì¹˜ ì¤‘ë³µë„ ë‚®ì€ í–‰ ì‚­ì œ\n",
    "train_data = delete_low_density(train_data, 2, 6)\n",
    "\n",
    "# built_yearê°€ 2024ì¸ í–‰ ì‚­ì œ\n",
    "train_data = train_data[train_data[\"built_year\"] < 2024]\n",
    "train_data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average Deposit by Region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í´ëŸ¬ìŠ¤í„°ë³„(region) í‰ê·  ì „ì„¸ê°€ ìƒì„±\n",
    "region_mean_prices = train_data.groupby(\"region\")[\"deposit\"].mean().reset_index()\n",
    "region_mean_prices.columns = [\"region\", \"mean_deposit\"]\n",
    "region_mean_prices[\"mean_deposit_category\"] = region_mean_prices[\"mean_deposit\"] // 10000\n",
    "\n",
    "# train_dataì™€ test_dataì— region_mean_prices ë³‘í•© (testì—ëŠ” trainì˜ í‰ê· ê°€ê²©ì´ ë³‘í•©ëœë‹¤.)\n",
    "train_data = train_data.merge(region_mean_prices, on=\"region\", how=\"left\")\n",
    "test_data = test_data.merge(region_mean_prices, on=\"region\", how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Log Transformation\n",
    "\n",
    "- `deposit`\n",
    "- `area_m2`\n",
    "- `nearest_subway_distance`\n",
    "- `nearest_school_distance`\n",
    "- `nearest_park_distance`\n",
    "- `nearest_leader_distance`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/pandas/core/internals/blocks.py:393: RuntimeWarning: divide by zero encountered in log1p\n",
      "  result = func(self.values, **kwargs)\n",
      "/opt/conda/lib/python3.10/site-packages/pandas/core/internals/blocks.py:393: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = func(self.values, **kwargs)\n",
      "/opt/conda/lib/python3.10/site-packages/pandas/core/internals/blocks.py:393: RuntimeWarning: divide by zero encountered in log1p\n",
      "  result = func(self.values, **kwargs)\n",
      "/opt/conda/lib/python3.10/site-packages/pandas/core/internals/blocks.py:393: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = func(self.values, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = apply_log_transformation(train_data, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3422478/3176361316.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  y.drop(columns=\"log_deposit\", inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# train data split\n",
    "X, y = split_features_and_target(train_data)\n",
    "\n",
    "# 1. ì›ë˜ ë°ì´í„° ì“°ëŠ” ì‚¬ëŒìš© (ë¶ˆí•„ìš”í•œ ë³€ìˆ˜ drop: ë¡œê·¸ë³€í™˜í•œ ë³€ìˆ˜)\n",
    "X.drop(columns=[\"index\", \"log_area_m2\", \"log_subway_distance\", \"log_school_distance\", \"log_park_distance\", \"log_leader_distance\"], inplace=True)\n",
    "y.drop(columns=\"log_deposit\", inplace=True)\n",
    "\n",
    "# 2. ë¡œê·¸ë³€í™˜í•œ ë°ì´í„° ì“°ëŠ” ì‚¬ëŒìš© (ë¶ˆí•„ìš”í•œ ë³€ìˆ˜ drop: ë¡œê·¸ë³€í™˜í•˜ê¸° ì „ ë³€ìˆ˜)\n",
    "#X.drop(columns=[\"index\", \"area_m2\", \"nearest_subway_distance\", \"nearest_school_distance\", \"nearest_park_distance\", \"nearest_leader_distance\"], inplace=True)\n",
    "#y.drop(columns=\"deposit\", inplace=True)\n",
    "\n",
    "# ì˜ê·  ì»¬ëŸ¼\n",
    "# selected_cols = [\n",
    "#    \"log_area_m2\", \"built_year\", \"latitude\", \"longitude\", \"log_leader_distance\", \"log_subway_distance\", \"log_park_distance\", \"contract_year_month\", \"num_of_subways_within_radius\", \"park_exists\", \"region\"\n",
    "# ]\n",
    "# X = X[selected_cols]\n",
    "# y.drop(columns=\"deposit\", inplace=True)\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=42)\n",
    "X_train = X_train.values\n",
    "X_valid = X_valid.values\n",
    "y_train = y_train.values\n",
    "y_valid = y_valid.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vanilla TabNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_model(model, X: pd.DataFrame, y: pd.DataFrame) -> float:\n",
    "#     \"\"\"ëª¨ë¸ì„ í•™ìŠµí•˜ê³  ê²€ì¦ MAEë¥¼ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.\n",
    "\n",
    "#     Args:\n",
    "#         model: ìˆ˜í–‰í•˜ë ¤ëŠ” ëª¨ë¸\n",
    "#         X (pd.DataFrame): ë…ë¦½ ë³€ìˆ˜\n",
    "#         y (pd.DataFrame): ì˜ˆì¸¡ ë³€ìˆ˜. depositê³¼ log_deposit ì—´ë¡œ ë‚˜ë‰¨.\n",
    "\n",
    "#     Returns:\n",
    "#         float: ê²€ì¦ MAE\n",
    "#     \"\"\"\n",
    "#     X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=42)\n",
    "#     X_train = X_train.values\n",
    "#     X_valid = X_valid.values\n",
    "#     y_train = y_train.values\n",
    "#     y_valid = y_valid.values\n",
    "\n",
    "#     # ëª¨ë¸ í•™ìŠµ\n",
    "#     model.fit(X_train, y_train, \n",
    "#           eval_set=[(X_train, y_train),(X_valid, y_valid)], \n",
    "#           eval_name=[\"train\", \"valid\"],\n",
    "#           eval_metric=[\"mae\"],\n",
    "#           loss_fn=torch.nn.L1Loss(),\n",
    "#           max_epochs=30, \n",
    "#           patience=10,\n",
    "#           batch_size=8192,\n",
    "#           drop_last=False,\n",
    "#           warm_start=True  # warm start í™œì„±í™”\n",
    "#     )\n",
    "#     print(\"ëª¨ë¸ í•™ìŠµì´ ì™„ë£ŒëìŠµë‹ˆë‹¤. â²\")\n",
    "\n",
    "#     # 2. ë¡œê·¸ë³€í™˜í•œ ë°ì´í„° ì“°ëŠ” ì‚¬ëŒìš©\n",
    "#     y_train = np.expm1(y_train) # -> ë¡œê·¸ë³€í™˜ ë³€ìˆ˜ ì‚¬ìš©ì‹œ í™œì„±í™”\n",
    "#     y_valid = np.expm1(y_valid) # -> ë¡œê·¸ë³€í™˜ ë³€ìˆ˜ ì‚¬ìš©ì‹œ í™œì„±í™”\n",
    "\n",
    "#     # ì˜ˆì¸¡ ë° ë¡œê·¸ ë³€í™˜ ë³µêµ¬\n",
    "#     y_train_pred = model.predict(X_train)\n",
    "#     y_train_pred = np.expm1(y_train_pred) # 2. ë¡œê·¸ë³€í™˜í•œ ë°ì´í„° ì“°ëŠ” ì‚¬ëŒìš© -> log_depositì˜ inverse log ì²˜ë¦¬\n",
    "#     y_valid_pred = model.predict(X_valid)\n",
    "#     y_valid_pred = np.expm1(y_valid_pred) # 2. ë¡œê·¸ë³€í™˜í•œ ë°ì´í„° ì“°ëŠ” ì‚¬ëŒìš© -> log_depositì˜ inverse log ì²˜ë¦¬\n",
    "\n",
    "#     # í•™ìŠµ MAE, ê²€ì¦ MAE ê³„ì‚°\n",
    "#     mae_train = mean_absolute_error(y_train, y_train_pred)\n",
    "#     mae_valid = mean_absolute_error(y_valid, y_valid_pred)\n",
    "#     print(\"í•™ìŠµ ê²°ê³¼..! ğŸ‰\")\n",
    "#     print(f\"Train MAE: {mae_train:.4f}, Valid MAE: {mae_valid:.4f}\")\n",
    "\n",
    "#     return mae_train, mae_valid\n",
    "\n",
    "\n",
    "# def objective(trial):\n",
    "#     \"\"\"\n",
    "# \t    Optunaë¥¼ ì´ìš©í•˜ì—¬ Hyperparameter íŠœë‹ì„ ìˆ˜í–‰í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.\n",
    "# \t  \"\"\"\n",
    "#     # n_dë¥¼ ë¨¼ì € ì œì•ˆí•©ë‹ˆë‹¤.\n",
    "#     n_d = trial.suggest_int(\"n_d\", 8, 64)\n",
    "#     params = {\n",
    "#             \"n_d\": n_d,\n",
    "#             \"n_a\": n_d,  # n_aëŠ” n_dì™€ ë™ì¼í•˜ê²Œ ì„¤ì •\n",
    "#             \"n_steps\": trial.suggest_int(\"n_steps\", 3, 10),\n",
    "#             \"gamma\": trial.suggest_float(\"gamma\", 1.0, 2.0),\n",
    "#             \"n_independent\": 2, # í•„ìš”í•˜ë©´ 3, 4ë¡œ ëŠ˜ë ¤ë³¸ë‹¤\n",
    "#             \"n_shared\": 2, # í•„ìš”í•˜ë©´ 3, 4ë¡œ ëŠ˜ë ¤ë³¸ë‹¤\n",
    "#             \"lambda_sparse\": trial.suggest_float(\"lambda_sparse\", 0.001, 0.01),\n",
    "#             \"optimizer_fn\": torch.optim.Adam,\n",
    "#             \"optimizer_params\": dict(lr=trial.suggest_float(\"learning_rate\", 0.001, 0.01)),\n",
    "#             \"verbose\": 1,\n",
    "#             \"device_name\" : \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "#             \"seed\" : 42\n",
    "#     }\n",
    "\n",
    "#     # TabNet ëª¨ë¸ ìƒì„±\n",
    "#     model = TabNetRegressor(**params)\n",
    "    \n",
    "#     # ëª¨ë¸ í•™ìŠµ ë° MAE ê³„ì‚°\n",
    "#     mae_train, mae_valid = train_model(model, X, y)\n",
    "#     ### ì‹œê°í™”ë¥¼ ì›í•˜ë©´ ì—¬ê¸°ì— ë„£ì–´ì£¼ì„¸ìš”. ###\n",
    "#     print(\"Optuna ê²°ê³¼..! ğŸ’«\")\n",
    "#     print(f\"Trial {trial.number}: Train MAE: {mae_train:.4f}, Valid MAE: {mae_valid:.4f}\")\n",
    "    \n",
    "#     return mae_valid #, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ì‹¤í—˜ ì‹œê°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ëª¨ë¸ì´ í•™ìŠµí•œ íˆìŠ¤í† ë¦¬ í™•ì¸\n",
    "# model.history.history.keys()\n",
    "\n",
    "\n",
    "# fig = plt.figure(figsize=(10, 5))\n",
    "\n",
    "# # ì†ì‹¤ ê·¸ë˜í”„ ê·¸ë¦¬ê¸°\n",
    "# plt.plot(model.history['loss'])\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.title('Training Loss Over Epochs')\n",
    "# plt.show()\n",
    "\n",
    "# # í•™ìŠµë¥  ê·¸ë˜í”„ ê·¸ë¦¬ê¸° (í˜„ì¬ëŠ” í•™ìŠµë¥ ì´ ê³ ì •ì´ë¼ ì§ì„ ì…ë‹ˆë‹¤.)\n",
    "# # model.historyì—ì„œ learning rate ê°’ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.\n",
    "# lr: list[float] = model.history['lr']\n",
    "# epochs: range = range(1, len(lr) + 1)\n",
    "\n",
    "# # learning rate ê·¸ë˜í”„ë¥¼ ê·¸ë¦½ë‹ˆë‹¤.\n",
    "# fig = plt.figure(figsize=(10, 5))\n",
    "# plt.plot(epochs, lr, color='green')\n",
    "# plt.xlabel(\"Epoch\")\n",
    "# plt.ylabel(\"Learning Rate\")\n",
    "# plt.title(\"Learning Rate over Epochs\")\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # model.historyì—ì„œ train MAEì™€ valid MAE ê°’ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.\n",
    "# train_auc: list[float] = model.history['train_mae']\n",
    "# valid_auc: list[float] = model.history['valid_mae']\n",
    "# epochs: range = range(1, len(train_auc) + 1)\n",
    "\n",
    "# # train AUCì™€ valid AUC ê·¸ë˜í”„ë¥¼ ê·¸ë¦½ë‹ˆë‹¤.\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.plot(epochs, train_auc, label='Train MAE', color='blue')\n",
    "# plt.plot(epochs, valid_auc, label='Valid MAE', color='red')\n",
    "# plt.xlabel(\"Epoch\")\n",
    "# plt.ylabel(\"MAE\")\n",
    "# plt.legend()\n",
    "# plt.title(\"Train and Valid MAE over Epochs\")\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Optuna ì‹¤í—˜ ì„¸íŒ… ë° ì‹¤í–‰\n",
    "# sampler = optuna.samplers.TPESampler(seed=42)\n",
    "# study = optuna.create_study(direction=\"minimize\", sampler=sampler)\n",
    "# study.optimize(objective, n_trials=10)\n",
    "\n",
    "# # ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¶œë ¥\n",
    "# best_params = study.best_params\n",
    "# print(\"Best hyperparameters: \", best_params)\n",
    "# print(\"Best MAE: \", study.best_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 24422.34528| train_mae: 12627.04499| valid_mae: 12614.26329|  0:02:41s\n",
      "epoch 1  | loss: 9846.15008| train_mae: 8202.48601| valid_mae: 8206.81213|  0:05:23s\n",
      "epoch 2  | loss: 8027.71512| train_mae: 7239.30863| valid_mae: 7236.95592|  0:08:18s\n",
      "epoch 3  | loss: 7099.26364| train_mae: 6205.86109| valid_mae: 6204.70454|  0:11:13s\n",
      "epoch 4  | loss: 6489.0099| train_mae: 5917.80169| valid_mae: 5921.38333|  0:14:06s\n",
      "epoch 5  | loss: 6136.78871| train_mae: 5678.81688| valid_mae: 5694.92745|  0:17:04s\n",
      "epoch 6  | loss: 5914.27421| train_mae: 5378.17398| valid_mae: 5393.93553|  0:20:07s\n",
      "epoch 7  | loss: 5666.37635| train_mae: 5320.03138| valid_mae: 5346.07817|  0:23:08s\n",
      "epoch 8  | loss: 5530.00947| train_mae: 5125.90529| valid_mae: 5154.98683|  0:26:10s\n",
      "epoch 9  | loss: 5406.55442| train_mae: 5379.83802| valid_mae: 5403.17694|  0:29:08s\n",
      "epoch 10 | loss: 5291.59724| train_mae: 4992.29855| valid_mae: 5024.12225|  0:32:09s\n",
      "epoch 11 | loss: 5210.04842| train_mae: 4854.43865| valid_mae: 4886.9461|  0:35:09s\n",
      "epoch 12 | loss: 5155.24526| train_mae: 4872.27658| valid_mae: 4908.97789|  0:38:09s\n",
      "epoch 13 | loss: 5100.93915| train_mae: 4730.37282| valid_mae: 4769.76767|  0:41:08s\n",
      "epoch 14 | loss: 5041.17981| train_mae: 4751.01829| valid_mae: 4791.89522|  0:44:09s\n",
      "epoch 15 | loss: 5016.40555| train_mae: 4742.70454| valid_mae: 4789.76643|  0:47:08s\n",
      "epoch 16 | loss: 5032.03744| train_mae: 4710.16097| valid_mae: 4757.43405|  0:50:06s\n",
      "epoch 17 | loss: 5042.79817| train_mae: 4646.21581| valid_mae: 4691.71103|  0:53:01s\n",
      "epoch 18 | loss: 5011.75623| train_mae: 4850.55041| valid_mae: 4895.9944|  0:55:52s\n",
      "epoch 19 | loss: 5013.03852| train_mae: 4586.65982| valid_mae: 4628.97697|  0:58:47s\n",
      "epoch 20 | loss: 4928.88763| train_mae: 4730.03019| valid_mae: 4770.24723|  1:01:41s\n",
      "epoch 21 | loss: 4909.01173| train_mae: 4565.09166| valid_mae: 4615.19347|  1:04:34s\n",
      "epoch 22 | loss: 4850.13754| train_mae: 4592.19715| valid_mae: 4641.62092|  1:07:25s\n",
      "epoch 23 | loss: 4837.97428| train_mae: 4496.44232| valid_mae: 4551.56088|  1:10:19s\n",
      "epoch 24 | loss: 4798.17447| train_mae: 4620.26682| valid_mae: 4666.72226|  1:13:02s\n",
      "epoch 25 | loss: 4788.15575| train_mae: 4641.31466| valid_mae: 4694.10605|  1:15:44s\n",
      "epoch 26 | loss: 4774.68081| train_mae: 4796.42878| valid_mae: 4847.98303|  1:18:24s\n",
      "epoch 27 | loss: 4842.74365| train_mae: 4522.69527| valid_mae: 4574.2818|  1:21:04s\n",
      "epoch 28 | loss: 4758.53582| train_mae: 4547.22196| valid_mae: 4602.81759|  1:23:47s\n",
      "epoch 29 | loss: 4810.84784| train_mae: 4708.18271| valid_mae: 4756.62103|  1:26:28s\n",
      "epoch 30 | loss: 4793.10895| train_mae: 4594.10559| valid_mae: 4655.81527|  1:29:11s\n",
      "epoch 31 | loss: 4791.18056| train_mae: 4577.17283| valid_mae: 4622.07937|  1:32:05s\n",
      "epoch 32 | loss: 4773.9027| train_mae: 4463.89769| valid_mae: 4510.25598|  1:34:55s\n",
      "epoch 33 | loss: 4770.88801| train_mae: 4828.33218| valid_mae: 4875.08779|  1:37:48s\n",
      "epoch 34 | loss: 4651.11446| train_mae: 4428.8153| valid_mae: 4488.61456|  1:40:40s\n",
      "epoch 35 | loss: 4618.2313| train_mae: 4469.99355| valid_mae: 4527.3507|  1:43:34s\n",
      "epoch 36 | loss: 4603.71106| train_mae: 4428.00055| valid_mae: 4489.66308|  1:46:28s\n",
      "epoch 37 | loss: 4579.36771| train_mae: 4400.65981| valid_mae: 4463.94618|  1:49:20s\n",
      "epoch 38 | loss: 4575.20664| train_mae: 4447.04493| valid_mae: 4506.96613|  1:52:11s\n",
      "epoch 39 | loss: 4567.36991| train_mae: 4327.20683| valid_mae: 4391.63908|  1:55:03s\n",
      "epoch 40 | loss: 4557.83011| train_mae: 4606.75773| valid_mae: 4661.15513|  1:57:49s\n",
      "epoch 41 | loss: 4542.28341| train_mae: 4253.0875| valid_mae: 4324.96575|  2:00:40s\n",
      "epoch 42 | loss: 4529.96491| train_mae: 4224.0505| valid_mae: 4296.41679|  2:03:33s\n",
      "epoch 43 | loss: 4523.43115| train_mae: 4260.48689| valid_mae: 4336.60146|  2:06:24s\n",
      "epoch 44 | loss: 4510.38043| train_mae: 4482.82775| valid_mae: 4562.09331|  2:09:15s\n",
      "epoch 45 | loss: 4505.93511| train_mae: 5073.99304| valid_mae: 5118.9127|  2:12:10s\n",
      "epoch 46 | loss: 4503.01495| train_mae: 4462.4511| valid_mae: 4526.0804|  2:14:53s\n",
      "epoch 47 | loss: 4502.08764| train_mae: 4238.46119| valid_mae: 4315.83865|  2:17:35s\n",
      "epoch 48 | loss: 4493.82509| train_mae: 4274.5203| valid_mae: 4348.91854|  2:20:19s\n",
      "epoch 49 | loss: 4490.82096| train_mae: 4228.36822| valid_mae: 4310.818|  2:23:05s\n",
      "epoch 50 | loss: 4479.28442| train_mae: 4632.44083| valid_mae: 4696.97207|  2:25:48s\n",
      "epoch 51 | loss: 4470.91069| train_mae: 4233.86384| valid_mae: 4317.06706|  2:28:32s\n"
     ]
    }
   ],
   "source": [
    "best_params = {\n",
    "    \"n_d\": 62,\n",
    "    \"n_a\": 62,  # n_aëŠ” n_dì™€ ë™ì¼í•˜ê²Œ ì„¤ì •\n",
    "    \"n_steps\": 8,\n",
    "    \"gamma\": 1.2533699284830764,\n",
    "    \"n_independent\": 2, # í•„ìš”í•˜ë©´ 3, 4ë¡œ ëŠ˜ë ¤ë³¸ë‹¤.\n",
    "    \"n_shared\": 2, # í•„ìš”í•˜ë©´ 3, 4ë¡œ ëŠ˜ë ¤ë³¸ë‹¤.\n",
    "    \"lambda_sparse\": 0.009596303461374517,\n",
    "    \"optimizer_fn\": torch.optim.Adam,\n",
    "    \"optimizer_params\": dict(lr=0.009855066118782934),\n",
    "    \"verbose\": 1,\n",
    "    \"device_name\" : \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    \"seed\" : 42\n",
    "}\n",
    "# Optunaë¡œ íŠœë‹í•œ íŒŒë¼ë¯¸í„°ë¡œ ëª¨ë¸ ì¬í•™ìŠµ\n",
    "best_model = TabNetRegressor(**best_params)\n",
    "best_model.fit(X_train, y_train, \n",
    "          eval_set=[(X_train, y_train),(X_valid, y_valid)], \n",
    "          eval_name=[\"train\", \"valid\"],\n",
    "          eval_metric=[\"mae\"],\n",
    "          loss_fn=torch.nn.L1Loss(),\n",
    "          max_epochs=150, \n",
    "          patience=10,\n",
    "          batch_size=2048,\n",
    "          drop_last=False,\n",
    "          warm_start=True  # warm start í™œì„±í™”\n",
    ")\n",
    "print(\"ëª¨ë¸ í•™ìŠµì´ ì™„ë£ŒëìŠµë‹ˆë‹¤. â²\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate & Save File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RuntimeError: CUDA error: device-side assert triggered ì—ëŸ¬ëŠ” ì£¼ë¡œ GPUì—ì„œ ë°ì´í„° ì²˜ë¦¬ ì¤‘ ë°œìƒí•˜ëŠ” ì˜¤ë¥˜ë¡œ, ë‹¤ìŒê³¼ ê°™ì€ ì›ì¸ì´ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n",
    "\n",
    "- ì˜ëª»ëœ ì…ë ¥ ë°ì´í„° í˜•ì‹: ì…ë ¥ ë°ì´í„°ê°€ ëª¨ë¸ì´ ê¸°ëŒ€í•˜ëŠ” í˜•ì‹ê³¼ ì¼ì¹˜í•˜ì§€ ì•Šê±°ë‚˜ ì˜ëª»ëœ ê°’ì„ í¬í•¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, TabNet ëª¨ë¸ì— ëŒ€í•œ ì…ë ¥ ë°ì´í„°ëŠ” float íƒ€ì…ì´ì–´ì•¼ í•˜ë©°, ì •ìˆ˜ ì¸ë±ìŠ¤ ë˜ëŠ” NaN ê°’ì´ ì—†ì–´ì•¼ í•©ë‹ˆë‹¤.\n",
    "\n",
    "- íƒ€ê²Ÿ ë³€ìˆ˜ì˜ ë²”ìœ„ ë¬¸ì œ: ì˜ˆì¸¡í•  ë•Œ íƒ€ê²Ÿ ë³€ìˆ˜ê°€ ì˜ˆìƒ ë²”ìœ„ë¥¼ ë²—ì–´ë‚˜ëŠ” ê²½ìš°ì—ë„ ì´ëŸ° ì˜¤ë¥˜ê°€ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "- ë°°ì¹˜ í¬ê¸° ë¬¸ì œ: ë°°ì¹˜ í¬ê¸°ê°€ ë„ˆë¬´ í¬ê±°ë‚˜ ë„ˆë¬´ ì‘ì•„ì„œ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "- CUDA ë“œë¼ì´ë²„ ë¬¸ì œ: ì‚¬ìš© ì¤‘ì¸ CUDA ë²„ì „ì´ PyTorchì™€ í˜¸í™˜ë˜ì§€ ì•Šê±°ë‚˜, ë“œë¼ì´ë²„ ì—…ë°ì´íŠ¸ê°€ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CUDA ëŸ°íƒ€ì„ ë¸”ë¡ ì„¤ì •**\n",
    "ì˜¤ë¥˜ ìœ„ì¹˜ ì •í™•íˆ í™•ì¸í•˜ê¸° ìœ„í•´ í™˜ê²½ ë³€ìˆ˜ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì…ë ¥ ë°ì´í„° í™•ì¸ -> floatíƒ€ì…, NaNê°’ì´ ì—†ì–´ì•¼ í•¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "interest_rateì— ê²°ì¸¡ì¹˜ í™•ì¸ -> meanëŒ€ì²´"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # interest_rate ì»¬ëŸ¼ì˜ ê²°ì¸¡ê°’ì„ í‰ê· ê°’ìœ¼ë¡œ ëŒ€ì²´\n",
    "test_data[\"interest_rate\"].fillna(test_data[\"interest_rate\"].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. ì›ë˜ ë°ì´í„° ì“°ëŠ” ì‚¬ëŒìš© (test_dataì—ì„œ ë¶ˆí•„ìš”í•œ ë³€ìˆ˜ drop: ë¡œê·¸ë³€í™˜í•œ ë³€ìˆ˜)\n",
    "X_test = test_data.drop(columns=[\"index\", \"log_area_m2\", \"log_subway_distance\", \"log_school_distance\", \"log_park_distance\", \"log_leader_distance\"], inplace=True)\n",
    "# 2. ë¡œê·¸ë³€í™˜í•œ ë°ì´í„° ì“°ëŠ” ì‚¬ëŒìš© (test_dataì—ì„œ ë¶ˆí•„ìš”í•œ ë³€ìˆ˜ drop: ë¡œê·¸ë³€í™˜í•˜ê¸° ì „ ë³€ìˆ˜)\n",
    "#X_test = test_data.drop(columns=[\"index\", \"area_m2\", \"nearest_subway_distance\", \"nearest_school_distance\", \"nearest_park_distance\", \"nearest_leader_distance\"], inplace=True)\n",
    "#X_test = X_test.values.astype(np.float32) # float32 ë„˜íŒŒì´ ë°°ì—´ë¡œ ë³€í™˜\n",
    "# X_test = test_data[selected_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test_data.values.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_testì— ëŒ€í•œ ì˜ˆì¸¡ ìˆ˜í–‰ í›„, ì˜ˆì¸¡ ê²°ê³¼ë¥¼ csv íŒŒì¼ë¡œ ì €ì¥\n",
    "# 1. ì›ë˜ ë°ì´í„° ì“°ëŠ” ì‚¬ëŒìš©\n",
    "\n",
    "y_pred = best_model.predict(X_test)\n",
    "#y_pred = np.expm1(y_pred)\n",
    "sample_submission[\"deposit\"] = y_pred\n",
    "sample_submission.to_csv(\"output.csv\", index=False)\n",
    "\n",
    "# 2. ë¡œê·¸ë³€í™˜í•œ ë°ì´í„° ì“°ëŠ” ì‚¬ëŒìš©\n",
    "#save_csv(best_model, X_test, sample_submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"deposit\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(y_pred.flatten()).describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

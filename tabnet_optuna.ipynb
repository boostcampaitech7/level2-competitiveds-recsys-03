{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from data.load_dataset import load_dataset\n",
    "from data.merge_dataset import merge_dataset\n",
    "from data.feature_engineering import *\n",
    "from model.inference import save_csv\n",
    "from model.feature_select import select_features\n",
    "from model.data_split import split_features_and_target\n",
    "from model.model_train import set_model, optuna_train\n",
    "#from model.TreeModel import XGBoost\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "import optuna\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기존 데이터 불러오기\n",
    "train_data, test_data, sample_submission, interest_data, subway_data, school_data, park_data = load_dataset()\n",
    "# 기존 데이터에 새로운 feature들을 병합한 데이터프레임 불러오기\n",
    "train_data, test_data = merge_dataset(train_data, test_data, interest_data, subway_data, school_data, park_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위치 중복도 낮은 행 삭제\n",
    "groups = train_data.groupby([\"latitude\", \"longitude\"])[\"index\"].count()\n",
    "conditioned_groups_index = groups[(groups >= 2) & (groups <= 5)].index # 이 범위를 파라미터로 조정하는걸로\n",
    "small_groups = train_data[\n",
    "    train_data[\"latitude\"].isin(conditioned_groups_index.get_level_values(0)) &\n",
    "    train_data[\"longitude\"].isin(conditioned_groups_index.get_level_values(1))\n",
    "]\n",
    "train_data.drop(small_groups.index, axis=0, inplace=True)\n",
    "\n",
    "# built_year > 2024 행 삭제\n",
    "train_data = train_data[train_data[\"built_year\"] < 2024]\n",
    "train_data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Clustering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 준비\n",
    "feature_columns = [\"latitude\", \"longitude\"]\n",
    "coords = train_data[feature_columns]\n",
    "\n",
    "# ClusteringModel 클래스 인스턴스 생성\n",
    "clustering_model = ClusteringModel(data=coords)\n",
    "kmeans_model = clustering_model.kmeans_clustering(n_clusters=25, \n",
    "                                                train_data=train_data, \n",
    "                                                test_data=test_data, \n",
    "                                                feature_columns=feature_columns, \n",
    "                                                label_column=\"region\")\n",
    "\n",
    "region_mean_prices = train_data.groupby(\"region\")[\"deposit\"].mean().reset_index()\n",
    "region_mean_prices.columns = [\"region\", \"mean_deposit\"]\n",
    "region_mean_prices[\"mean_deposit_category\"] = region_mean_prices[\"mean_deposit\"] // 10000\n",
    "\n",
    "# train_data와 region_mean_prices 병합\n",
    "train_data = train_data.merge(region_mean_prices, on=\"region\", how=\"left\")\n",
    "test_data = test_data.merge(region_mean_prices, on=\"region\", how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Log변환**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/pandas/core/internals/blocks.py:393: RuntimeWarning: divide by zero encountered in log1p\n",
      "  result = func(self.values, **kwargs)\n",
      "/opt/conda/lib/python3.10/site-packages/pandas/core/internals/blocks.py:393: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = func(self.values, **kwargs)\n",
      "/opt/conda/lib/python3.10/site-packages/pandas/core/internals/blocks.py:393: RuntimeWarning: divide by zero encountered in log1p\n",
      "  result = func(self.values, **kwargs)\n",
      "/opt/conda/lib/python3.10/site-packages/pandas/core/internals/blocks.py:393: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = func(self.values, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = apply_log_transformation(train_data, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**train_data split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = split_features_and_target(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature select**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X, test_data = select_features(X, y, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['index', 'area_m2', 'contract_year_month', 'contract_day',\n",
       "       'contract_type', 'floor', 'built_year', 'latitude', 'longitude', 'age',\n",
       "       'interest_rate', 'nearest_subway_distance', 'nearest_subway_latitude',\n",
       "       'nearest_subway_longitude', 'nearest_school_distance',\n",
       "       'nearest_school_latitude', 'nearest_school_longitude',\n",
       "       'nearest_park_distance', 'nearest_park_latitude',\n",
       "       'nearest_park_longitude', 'nearest_subway_num', 'nearest_school_num',\n",
       "       'nearest_park_num', 'num_of_subways_within_radius',\n",
       "       'num_of_schools_within_radius', 'num_of_parks_within_radius',\n",
       "       'park_exists', 'region', 'region_mean', 'nearest_leader_distance',\n",
       "       'nearest_leader_latitude', 'nearest_leader_longitude', 'mean_deposit',\n",
       "       'mean_deposit_category', 'log_area_m2', 'log_subway_distance',\n",
       "       'log_school_distance', 'log_park_distance', 'log_leader_distance'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['deposit', 'log_deposit'], dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1790125, 39)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1790125, 2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Train and Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tabnet**\n",
    "- 테이블 데이터에서도 딥러닝이 잘 동작할 수 있게 만들어진 모델\n",
    "- 자동으로 중요한 features를 선택하기 떄문에 feature select부분은 제외"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model_train.py 따라한 ver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import mean_absolute_error\n",
    "# from sklearn.model_selection import KFold\n",
    "\n",
    "# def cv_train(model, X: pd.DataFrame, y: pd.DataFrame, verbose: bool = True) -> float:\n",
    "#     \"\"\"K-Fold를 이용하여 Cross Validation을 수행하는 함수입니다.\n",
    "\n",
    "#     Args:\n",
    "#         model: 수행하려는 모델\n",
    "#         X (pd.DataFrame): 독립 변수\n",
    "#         y (pd.DataFrame): 예측 변수. deposit과 log_deposit 열로 나뉨.\n",
    "#         verbose (bool, optional): Fold별 진행상황을 출력할지 여부. Defaults to True.\n",
    "\n",
    "#     Returns:\n",
    "#         float: 평균 MAE\n",
    "#     \"\"\"\n",
    "#     cv = 5\n",
    "#     kfold = KFold(n_splits=cv, shuffle=True, random_state=42)\n",
    "\n",
    "#     mae_list = []\n",
    "#     for i, (train_idx, valid_idx) in enumerate(kfold.split(X, y), start=1):\n",
    "#         if verbose: print(f\"training...[{i}/{cv}]\")\n",
    "\n",
    "#         X_train, y_train = X.loc[train_idx, :].values, y.loc[train_idx, \"log_deposit\"].values.reshape(-1, 1)\n",
    "#         X_valid, y_valid = X.loc[valid_idx, :].values, y.loc[valid_idx, \"deposit\"].values.reshape(-1, 1)\n",
    "\n",
    "#         model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], eval_metric=[\"mae\"])\n",
    "\n",
    "#         y_pred = model.predict(X_valid)\n",
    "#         y_pred = np.expm1(y_pred)\n",
    "#         fold_mae = mean_absolute_error(y_valid, y_pred)\n",
    "#         if verbose: print(f\"Valid MAE: {fold_mae:.4f}\")\n",
    "#         mae_list.append(fold_mae)\n",
    "\n",
    "#     mae = np.mean(mae_list)\n",
    "#     if verbose:\n",
    "#         print(\"### K-fold Result ###\")\n",
    "#         print(f\"Valid MAE: {mae:.4f}\")\n",
    "    \n",
    "#     return mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def objective(trial):\n",
    "#     params = {\n",
    "#         \"n_d\": trial.suggest_int(\"n_d\", 16, 64),\n",
    "#         \"n_steps\": trial.suggest_int(\"n_steps\", 3, 10),\n",
    "#         \"gamma\": trial.suggest_float(\"gamma\", 1.0, 2.0),\n",
    "#         \"lambda_sparse\": trial.suggest_float(\"lambda_sparse\", 0.0001, 0.01),\n",
    "#         \"optimizer_fn\": torch.optim.Adam,\n",
    "#         \"optimizer_params\": dict(lr=trial.suggest_float(\"learning_rate\", 0.01, 0.1)),\n",
    "#     }\n",
    "#     model = TabNetRegressor(**params)\n",
    "#     return cv_train(model, X, y, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "합친 ver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import mean_absolute_error\n",
    "# from sklearn.model_selection import KFold, cross_val_predict\n",
    "\n",
    "# def objective(trial):\n",
    "#     params = {\n",
    "#         \"n_d\": trial.suggest_int(\"n_d\", 8, 64),\n",
    "#         \"n_a\": trial.suggest_int(\"n_a\", 8, 64),\n",
    "#         \"n_steps\": trial.suggest_int(\"n_steps\", 3, 10),\n",
    "#         \"gamma\": trial.suggest_float(\"gamma\", 1.0, 2.0),\n",
    "#         \"lambda_sparse\": trial.suggest_float(\"lambda_sparse\", 0.0001, 0.01),\n",
    "#         \"optimizer_params\": dict(lr=trial.suggest_float(\"learning_rate\", 0.01, 0.1)),\n",
    "#     }\n",
    "    \n",
    "#     # K-Fold 교차 검증\n",
    "#     cv = 5\n",
    "#     kfold = KFold(n_splits=cv, shuffle=True, random_state=42)\n",
    "#     mae_list = []\n",
    "    \n",
    "#     for i, (train_idx, valid_idx) in enumerate(kfold.split(X, y), start=1):\n",
    "#         if True: print(f\"training...[{i}/{cv}]\")\n",
    "#         X_train, y_train = X.loc[train_idx, :].values, y.loc[train_idx, \"log_deposit\"].values.reshape(-1, 1)\n",
    "#         X_valid, y_valid = X.loc[valid_idx, :].values, y.loc[valid_idx, \"deposit\"].values.reshape(-1, 1)\n",
    "        \n",
    "#         model = TabNetRegressor(**params)\n",
    "#         # 모델 학습 (patience : 성능 개선되지않으면 early stopping)\n",
    "#         model.fit(\n",
    "#             X_train, y_train, \n",
    "#             eval_set=[(X_valid, y_valid)], \n",
    "#             eval_metric=[\"mae\"], \n",
    "#             max_epochs=100,\n",
    "#             patience=10,\n",
    "#             batch_size=1024,\n",
    "#             virtual_batch_size=128,\n",
    "#         )\n",
    "        \n",
    "        \n",
    "#         # 검증 데이터에 대한 예측\n",
    "#         y_pred = model.predict(X_valid.values)\n",
    "#         y_pred = np.expm1(y_pred)  # 로그 변환의 역변환\n",
    "        \n",
    "#         # MAE 계산\n",
    "#         mae = mean_absolute_error(y_valid, y_pred) \n",
    "#         mae_list.append(mae)\n",
    "\n",
    "#     # 교차 검증 후 MAE 평균값 반환\n",
    "#     return np.mean(mae_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.metrics import mean_absolute_error\n",
    "# from sklearn.model_selection import KFold\n",
    "# from model.TreeModel import XGBoost, LightGBM, CatBoost\n",
    "# import optuna\n",
    "# RANDOM_SEED = 42\n",
    "\n",
    "# def set_model(model_name: str, **params):\n",
    "#     \"\"\"주어진 모델 이름에 따라 모델을 생성하고 반환하는 함수입니다.\n",
    "\n",
    "#     Args:\n",
    "#         model_name (str): 생성하려는 모델 이름\n",
    "#         **params (dict): 모델 생성 시 사용할 하이퍼파라미터\n",
    "\n",
    "#     Returns:\n",
    "#         model (object): 생성된 모델 객체\n",
    "#     \"\"\"\n",
    "#     match model_name:\n",
    "#         case \"xgboost\":\n",
    "#             model = XGBoost(**params)\n",
    "#         case \"lightgbm\":\n",
    "#             model = LightGBM(**params)\n",
    "#         case \"catboost\":\n",
    "#             model = CatBoost(**params)\n",
    "#         case \"tabnet\":\n",
    "#             model = TabNetRegressor(**params)\n",
    "#     return model\n",
    "\n",
    "# def cv_train(model, X: pd.DataFrame, y: pd.DataFrame, verbose: bool = True) -> float:\n",
    "#     \"\"\"K-Fold를 이용하여 Cross Validation을 수행하는 함수입니다.\n",
    "\n",
    "#     Args:\n",
    "#         model: 수행하려는 모델\n",
    "#         X (pd.DataFrame): 독립 변수\n",
    "#         y (pd.DataFrame): 예측 변수. deposit과 log_deposit 열로 나뉨.\n",
    "#         verbose (bool, optional): Fold별 진행상황을 출력할지 여부. Defaults to True.\n",
    "\n",
    "#     Returns:\n",
    "#         float: 평균 MAE\n",
    "#     \"\"\"\n",
    "#     cv = 5\n",
    "#     kfold = KFold(n_splits=cv, shuffle=True, random_state=42)\n",
    "\n",
    "#     mae_list = []\n",
    "#     for i, (train_idx, valid_idx) in enumerate(kfold.split(X, y), start=1):\n",
    "#         if verbose: print(f\"training...[{i}/{cv}]\")\n",
    "\n",
    "#         X_train, y_train = X.loc[train_idx, :].values, y.loc[train_idx, \"log_deposit\"].values.reshape(-1,1)\n",
    "#         X_valid, y_valid = X.loc[valid_idx, :].values, y.loc[valid_idx, \"deposit\"].values.reshape(-1,1)\n",
    "\n",
    "#         model.fit(\n",
    "#             X_train, y_train, \n",
    "#             eval_set=[(X_valid, y_valid)], \n",
    "#             eval_metric=[\"mae\"], \n",
    "#             max_epochs=100,\n",
    "#             patience=10\n",
    "#         )\n",
    "\n",
    "#         y_pred = model.predict(X_valid)\n",
    "#         y_pred = np.expm1(y_pred)\n",
    "#         fold_mae = mean_absolute_error(y_valid, y_pred)\n",
    "#         if verbose: print(f\"Valid MAE: {fold_mae:.4f}\")\n",
    "#         mae_list.append(fold_mae)\n",
    "\n",
    "#     mae = np.mean(mae_list)\n",
    "#     if verbose:\n",
    "#         print(\"### K-fold Result ###\")\n",
    "#         print(f\"Valid MAE: {mae:.4f}\")\n",
    "    \n",
    "#     return mae\n",
    "\n",
    "# def optuna_train(model_name: str, X: pd.DataFrame, y: pd.DataFrame) -> tuple[dict, float]:\n",
    "#     \"\"\"Optuna를 사용하여 주어진 모델의 하이퍼파라미터를 최적하는 함수\n",
    "\n",
    "#     Args:\n",
    "#         model_name (str): 최적화할 모델의 이름\n",
    "#         X (pd.DataFrame): 독립 변수\n",
    "#         y (pd.DataFrame): 예측 변수\n",
    "\n",
    "#     Returns:\n",
    "#         tuple[dict, float]:\n",
    "#             - dict: 최적의 하이퍼파라미터\n",
    "#             - float: 최적의 하이퍼파라미터에 대한 성능 지표(MAE)\n",
    "#     \"\"\"\n",
    "#     def objective(trial):\n",
    "#         match model_name:\n",
    "#             case \"xgboost\":\n",
    "#                 params = {\n",
    "#                     \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 300),\n",
    "#                     \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.2),\n",
    "#                     \"max_depth\": trial.suggest_int(\"max_depth\", 5, 12),\n",
    "#                     \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "#                 }\n",
    "#             case \"lightgbm\":\n",
    "#                 params = {\n",
    "#                     \"verbose\": -1,\n",
    "#                     \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 300),\n",
    "#                     \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True),\n",
    "#                     \"max_depth\": trial.suggest_int(\"max_depth\", 5, 12),\n",
    "#                     \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "#                     \"num_leaves\": trial.suggest_int(\"num_leaves\", 20, 150),\n",
    "#                     \"objective\": \"regression_l1\"\n",
    "#             }\n",
    "#             case \"catboost\":\n",
    "#                 params = {\n",
    "#                     \"verbose\": 0,\n",
    "#                     \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n",
    "#                     \"iterations\": trial.suggest_int(\"iterations\", 50, 500),\n",
    "#                     \"depth\": trial.suggest_int(\"depth\", 3, 10),\n",
    "#                     \"l2_leaf_reg\": trial.suggest_int(\"l2_leaf_reg\", 1, 10),\n",
    "#                     # \"bagging_temperature\": trial.suggest_loguniform(\"bagging_temperature\", 0.01, 1),\n",
    "#                     # \"border_count\": trial.suggest_int(\"border_count\", 32, 255),\n",
    "#                     \"cat_features\": [\"contract_day\"],\n",
    "#                     \"task_type\": \"GPU\",\n",
    "#                     \"devices\": \"cuda\",\n",
    "#                 }\n",
    "#             case \"tabnet\":\n",
    "#                 params = {\n",
    "#                     \"n_d\": trial.suggest_int(\"n_d\", 8, 64),\n",
    "#                     \"n_steps\": trial.suggest_int(\"n_steps\", 3, 10),\n",
    "#                     \"gamma\": trial.suggest_float(\"gamma\", 1.0, 2.0),\n",
    "#                     \"lambda_sparse\": trial.suggest_float(\"lambda_sparse\", 0.0001, 0.01),\n",
    "#                     \"optimizer_params\": dict(lr=trial.suggest_float(\"learning_rate\", 0.01, 0.1)),\n",
    "#                 }\n",
    "#         model = set_model(model_name, **params)\n",
    "#         return cv_train(model, X, y, verbose=False)\n",
    "    \n",
    "#     sampler = optuna.samplers.TPESampler(seed=42)\n",
    "#     study = optuna.create_study(direction=\"minimize\", sampler=sampler)\n",
    "#     study.optimize(objective, n_trials=50)\n",
    "#     return study.best_params, study.best_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "값이 너무 커서 hyperparameter 범위 조정 & kfold train,valid split부분 수정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from sklearn.metrics import mean_absolute_error\n",
    "# from sklearn.model_selection import KFold\n",
    "# import optuna\n",
    "# import torch\n",
    "# from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "\n",
    "# def cv_train(model, X: pd.DataFrame, y: pd.DataFrame, verbose: bool = True) -> float:\n",
    "#     \"\"\"K-Fold를 이용하여 Cross Validation을 수행하는 함수입니다.\n",
    "\n",
    "#     Args:\n",
    "#         model: 수행하려는 모델\n",
    "#         X (pd.DataFrame): 독립 변수\n",
    "#         y (pd.DataFrame): 예측 변수. deposit과 log_deposit 열로 나뉨.\n",
    "#         verbose (bool, optional): Fold별 진행상황을 출력할지 여부. Defaults to True.\n",
    "\n",
    "#     Returns:\n",
    "#         float: 평균 MAE\n",
    "#     \"\"\"\n",
    "#     cv = 5\n",
    "#     kfold = KFold(n_splits=cv, shuffle=True, random_state=42)\n",
    "\n",
    "#     mae_list = []\n",
    "#     for i, (train_idx, valid_idx) in enumerate(kfold.split(X, y), start=1):\n",
    "#         if verbose: print(f\"training...[{i}/{cv}]\")\n",
    "\n",
    "#         X_train, y_train = X.iloc[train_idx, :].values, y.iloc[train_idx, 1].values.reshape(-1, 1)\n",
    "#         X_valid, y_valid = X.iloc[valid_idx, :].values, y.iloc[valid_idx, 0].values.reshape(-1, 1)\n",
    "\n",
    "\n",
    "#         model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], eval_metric=[\"mae\"])\n",
    "\n",
    "#         y_pred = model.predict(X_valid)\n",
    "#         y_pred = np.expm1(y_pred)  # 로그 변환 복구\n",
    "        \n",
    "#         fold_mae = mean_absolute_error(y_valid, y_pred)\n",
    "#         if verbose: print(f\"Valid MAE: {fold_mae:.4f}\")\n",
    "#         mae_list.append(fold_mae)\n",
    "\n",
    "#     mae = np.mean(mae_list)\n",
    "#     if verbose:\n",
    "#         print(\"### K-fold Result ###\")\n",
    "#         print(f\"Valid MAE: {mae:.4f}\")\n",
    "    \n",
    "#     return mae\n",
    "\n",
    "# def objective(trial):\n",
    "#     params = {\n",
    "#         \"n_d\": trial.suggest_int(\"n_d\", 8, 64),\n",
    "#         'n_a': trial.suggest_int('n_a', 8, 64),\n",
    "#         \"n_steps\": trial.suggest_int(\"n_steps\", 3, 10),\n",
    "#         \"gamma\": trial.suggest_float(\"gamma\", 1.0, 2.0),\n",
    "#         'n_independent': trial.suggest_int('n_independent', 1, 5),\n",
    "#         'n_shared': trial.suggest_int('n_shared', 1, 5),\n",
    "#         \"lambda_sparse\": trial.suggest_float(\"lambda_sparse\", 0.0001, 0.01),\n",
    "#         \"optimizer_fn\": torch.optim.Adam,\n",
    "#         \"optimizer_params\": dict(lr=trial.suggest_float(\"learning_rate\", 0.001, 0.01)),\n",
    "#     }\n",
    "#     model = TabNetRegressor(**params)\n",
    "#     return cv_train(model, X, y, verbose=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampler = optuna.samplers.TPESampler(seed=42)\n",
    "# study = optuna.create_study(direction=\"minimize\", sampler=sampler)\n",
    "# study.optimize(objective, n_trials=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kfold 제거 ver 그러나 실패"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from sklearn.metrics import mean_absolute_error\n",
    "# import optuna\n",
    "# import torch\n",
    "# from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "\n",
    "# def train_model(model, X: pd.DataFrame, y: pd.DataFrame) -> float:\n",
    "#     \"\"\"모델을 학습하고 검증 MAE를 계산하는 함수입니다.\n",
    "\n",
    "#     Args:\n",
    "#         model: 수행하려는 모델\n",
    "#         X (pd.DataFrame): 독립 변수\n",
    "#         y (pd.DataFrame): 예측 변수. deposit과 log_deposit 열로 나뉨.\n",
    "\n",
    "#     Returns:\n",
    "#         float: 검증 MAE\n",
    "#     \"\"\"\n",
    "#     # 모델 학습\n",
    "#     model.fit(X.values, y.iloc[:, 0].values.reshape(-1, 1), eval_metric=[\"mae\"])\n",
    "\n",
    "#     # 예측 및 로그 변환 복구\n",
    "#     y_pred = model.predict(X.values)\n",
    "#     y_pred = np.expm1(y_pred)  # log_deposit의 inverse log 처리\n",
    "\n",
    "#     # 검증 MAE 계산\n",
    "#     mae = mean_absolute_error(y.iloc[:, 1], y_pred)  # deposit 열을 사용\n",
    "#     return mae\n",
    "\n",
    "# def objective(trial):\n",
    "#     \"\"\"Optuna를 이용하여 Hyperparameter 튜닝을 수행하는 함수입니다.\"\"\"\n",
    "#     params = {\n",
    "#         \"n_d\": trial.suggest_int(\"n_d\", 8, 64),\n",
    "#         \"n_a\": trial.suggest_int(\"n_a\", 8, 64),\n",
    "#         \"n_steps\": trial.suggest_int(\"n_steps\", 3, 10),\n",
    "#         \"gamma\": trial.suggest_float(\"gamma\", 1.0, 2.0),\n",
    "#         \"n_independent\": trial.suggest_int(\"n_independent\", 1, 5),\n",
    "#         \"n_shared\": trial.suggest_int(\"n_shared\", 1, 5),\n",
    "#         \"lambda_sparse\": trial.suggest_float(\"lambda_sparse\", 0.0001, 0.01),\n",
    "#         \"optimizer_fn\": torch.optim.Adam,\n",
    "#         \"optimizer_params\": dict(lr=trial.suggest_float(\"learning_rate\", 0.001, 0.01)),\n",
    "#     }\n",
    "\n",
    "#     # TabNet 모델 생성\n",
    "#     model = TabNetRegressor(**params)\n",
    "    \n",
    "#     # 모델 학습 및 MAE 계산\n",
    "#     mae = train_model(model, X, y)\n",
    "    \n",
    "#     return mae\n",
    "\n",
    "# # Optuna 실험 세팅 및 실행\n",
    "# sampler = optuna.samplers.TPESampler(seed=42)\n",
    "# study = optuna.create_study(direction=\"minimize\", sampler=sampler)\n",
    "# study.optimize(objective, n_trials=50)\n",
    "\n",
    "# # 최적 하이퍼파라미터 출력\n",
    "# best_params = study.best_params\n",
    "# print(\"Best hyperparameters: \", best_params)\n",
    "# print(\"Best MAE: \", study.best_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kfold 없애고 max_epoch=100 + GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "#from pytorch_tabnet.callbacks import Callback\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y[\"log_deposit\"], test_size=0.2, shuffle=True ,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class MAECallback(Callback):\n",
    "    # def on_epoch_end(self, epoch, logs=None):\n",
    "    #     logs = logs or {}\n",
    "    #     if 'valid_mae' in logs:\n",
    "    #         print(f\"Epoch {epoch+1} - Valid MAE: {logs['valid_mae']}\")\n",
    "\n",
    "def train_model(model, X: pd.DataFrame, y: pd.DataFrame) -> float:\n",
    "    \"\"\"모델을 학습하고 검증 MAE를 계산하는 함수입니다.\n",
    "\n",
    "    Args:\n",
    "        model: 수행하려는 모델\n",
    "        X (pd.DataFrame): 독립 변수\n",
    "        y (pd.DataFrame): 예측 변수. deposit과 log_deposit 열로 나뉨.\n",
    "\n",
    "    Returns:\n",
    "        float: 검증 MAE\n",
    "    \"\"\"\n",
    "    # 모델 학습\n",
    "    model.fit(\n",
    "            X_train.values, y_train.values.reshape(-1, 1),\n",
    "            eval_set=[(X_train.values, y_train.values.reshape(-1, 1)),(X_val.values, y_val.values.reshape(-1, 1))],\n",
    "            eval_name=['train', 'valid'],\n",
    "            eval_metric=[\"mae\"],\n",
    "            loss_fn=torch.nn.L1Loss(),\n",
    "            max_epochs=30, \n",
    "            patience=10,\n",
    "            batch_size=8192,\n",
    "            drop_last=False,\n",
    "            warm_start=True  # warm start 활성화\n",
    "    )\n",
    "\n",
    "    # 예측 및 로그 변환 복구\n",
    "    y_pred_log = model.predict(X_val.values)\n",
    "    y_pred = np.expm1(y_pred_log)  # log_deposit의 inverse log 처리\n",
    "\n",
    "    # 검증 MAE 계산\n",
    "    y_val_actual = np.expm1(y_val.values)\n",
    "    mae = mean_absolute_error(y_val_actual, y_pred)  # deposit 열을 사용\n",
    "    return mae\n",
    "\n",
    "def objective(trial):\n",
    "    \"\"\"Optuna를 이용하여 Hyperparameter 튜닝을 수행하는 함수입니다.\"\"\"\n",
    "    # n_d를 먼저 제안합니다.\n",
    "    n_d = trial.suggest_int(\"n_d\", 8, 64)\n",
    "    params = {\n",
    "            \"n_d\": n_d,\n",
    "            \"n_a\": n_d,  # n_a는 n_d와 동일하게 설정\n",
    "            \"n_steps\": trial.suggest_int(\"n_steps\", 3, 10),\n",
    "            \"gamma\": trial.suggest_float(\"gamma\", 1.0, 2.0),\n",
    "            \"n_independent\": 2, # 필요하면 3, 4로 늘려본다\n",
    "            \"n_shared\": 2, # 필요하면 3, 4로 늘려본다\n",
    "            \"lambda_sparse\": trial.suggest_float(\"lambda_sparse\", 0.001, 0.01),\n",
    "            \"optimizer_fn\": torch.optim.Adam,\n",
    "            \"optimizer_params\": dict(lr=trial.suggest_float(\"learning_rate\", 0.001, 0.01)),\n",
    "            \"verbose\": 1,\n",
    "            \"device_name\" : \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "            \"seed\" : 42\n",
    "    }\n",
    "\n",
    "    # TabNet 모델 생성\n",
    "    model = TabNetRegressor(**params)\n",
    "    \n",
    "    # 모델 학습 및 MAE 계산\n",
    "    mae = train_model(model, X, y)\n",
    "    print(f\"Trial {trial.number}: MAE = {mae}\")\n",
    "    \n",
    "    return mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optuna 실험 세팅 및 실행\n",
    "sampler = optuna.samplers.TPESampler(seed=42)\n",
    "study = optuna.create_study(direction=\"minimize\", sampler=sampler)\n",
    "study.optimize(objective, n_trials=10)\n",
    "\n",
    "# 최적 하이퍼파라미터 출력\n",
    "best_params = study.best_params\n",
    "print(\"Best hyperparameters: \", best_params)\n",
    "print(\"Best MAE: \", study.best_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_params = study.best_params\n",
    "# print(\"Best parameters for Tabnet: \", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.50878 | train_mae: 138.04847| valid_mae: 138.0774|  0:01:49s\n",
      "epoch 1  | loss: 0.28786 | train_mae: 16.48729| valid_mae: 16.49411|  0:03:37s\n",
      "epoch 2  | loss: 0.25177 | train_mae: 1.72534 | valid_mae: 1.71847 |  0:05:24s\n",
      "epoch 3  | loss: 0.2341  | train_mae: 0.30367 | valid_mae: 0.30375 |  0:07:12s\n",
      "epoch 4  | loss: 0.22398 | train_mae: 0.21018 | valid_mae: 0.2099  |  0:08:59s\n",
      "epoch 5  | loss: 0.21568 | train_mae: 0.19938 | valid_mae: 0.19905 |  0:10:46s\n",
      "epoch 6  | loss: 0.2095  | train_mae: 0.19614 | valid_mae: 0.19607 |  0:12:33s\n",
      "epoch 7  | loss: 0.20657 | train_mae: 0.1894  | valid_mae: 0.18946 |  0:14:20s\n",
      "epoch 8  | loss: 0.19955 | train_mae: 0.18907 | valid_mae: 0.1892  |  0:16:08s\n",
      "epoch 9  | loss: 0.19978 | train_mae: 0.18605 | valid_mae: 0.18594 |  0:17:55s\n",
      "epoch 10 | loss: 0.19775 | train_mae: 0.18399 | valid_mae: 0.18397 |  0:19:44s\n",
      "epoch 11 | loss: 0.19124 | train_mae: 0.17838 | valid_mae: 0.1786  |  0:21:34s\n",
      "epoch 12 | loss: 0.18928 | train_mae: 0.17997 | valid_mae: 0.18003 |  0:23:22s\n",
      "epoch 13 | loss: 0.18777 | train_mae: 0.18287 | valid_mae: 0.18305 |  0:25:10s\n",
      "epoch 14 | loss: 0.18675 | train_mae: 0.17594 | valid_mae: 0.17599 |  0:27:00s\n",
      "epoch 15 | loss: 0.18735 | train_mae: 0.17498 | valid_mae: 0.17494 |  0:28:48s\n",
      "epoch 16 | loss: 0.18455 | train_mae: 0.17184 | valid_mae: 0.17195 |  0:30:37s\n",
      "epoch 17 | loss: 0.1811  | train_mae: 0.17052 | valid_mae: 0.17066 |  0:32:29s\n",
      "epoch 18 | loss: 0.18071 | train_mae: 0.1666  | valid_mae: 0.16673 |  0:34:17s\n",
      "epoch 19 | loss: 0.1796  | train_mae: 0.17162 | valid_mae: 0.17187 |  0:36:05s\n",
      "epoch 20 | loss: 0.17716 | train_mae: 0.16776 | valid_mae: 0.16812 |  0:37:53s\n",
      "epoch 21 | loss: 0.17523 | train_mae: 0.16354 | valid_mae: 0.16395 |  0:39:39s\n",
      "epoch 22 | loss: 0.17313 | train_mae: 0.16536 | valid_mae: 0.16574 |  0:41:25s\n",
      "epoch 23 | loss: 0.17095 | train_mae: 0.15873 | valid_mae: 0.15915 |  0:43:10s\n",
      "epoch 24 | loss: 0.16966 | train_mae: 0.15887 | valid_mae: 0.1593  |  0:44:56s\n",
      "epoch 25 | loss: 0.16912 | train_mae: 0.15737 | valid_mae: 0.15779 |  0:46:41s\n",
      "epoch 26 | loss: 0.16804 | train_mae: 0.15707 | valid_mae: 0.15766 |  0:48:27s\n",
      "epoch 27 | loss: 0.1666  | train_mae: 0.15646 | valid_mae: 0.15692 |  0:50:13s\n",
      "epoch 28 | loss: 0.16896 | train_mae: 0.15724 | valid_mae: 0.15789 |  0:51:57s\n",
      "epoch 29 | loss: 0.1653  | train_mae: 0.15622 | valid_mae: 0.1567  |  0:53:42s\n",
      "epoch 30 | loss: 0.16404 | train_mae: 0.15425 | valid_mae: 0.15487 |  0:55:26s\n",
      "epoch 31 | loss: 0.16423 | train_mae: 0.15125 | valid_mae: 0.15188 |  0:57:12s\n",
      "epoch 32 | loss: 0.16145 | train_mae: 0.15191 | valid_mae: 0.15256 |  0:58:57s\n",
      "epoch 33 | loss: 0.1597  | train_mae: 0.14766 | valid_mae: 0.14846 |  1:00:42s\n",
      "epoch 34 | loss: 0.15796 | train_mae: 0.14694 | valid_mae: 0.14782 |  1:02:26s\n",
      "epoch 35 | loss: 0.15738 | train_mae: 0.14538 | valid_mae: 0.1461  |  1:04:12s\n",
      "epoch 36 | loss: 0.15607 | train_mae: 0.14499 | valid_mae: 0.1457  |  1:05:58s\n",
      "epoch 37 | loss: 0.15541 | train_mae: 0.14703 | valid_mae: 0.14781 |  1:07:43s\n",
      "epoch 38 | loss: 0.15452 | train_mae: 0.14294 | valid_mae: 0.14377 |  1:09:28s\n",
      "epoch 39 | loss: 0.1535  | train_mae: 0.14439 | valid_mae: 0.14533 |  1:11:16s\n",
      "epoch 40 | loss: 0.1533  | train_mae: 0.14077 | valid_mae: 0.14167 |  1:13:01s\n",
      "epoch 41 | loss: 0.15394 | train_mae: 0.14474 | valid_mae: 0.14564 |  1:14:46s\n",
      "epoch 42 | loss: 0.15248 | train_mae: 0.14245 | valid_mae: 0.14342 |  1:16:31s\n",
      "epoch 43 | loss: 0.15268 | train_mae: 0.14696 | valid_mae: 0.14766 |  1:18:16s\n",
      "epoch 44 | loss: 0.15345 | train_mae: 0.14121 | valid_mae: 0.14214 |  1:20:01s\n",
      "epoch 45 | loss: 0.15005 | train_mae: 0.14153 | valid_mae: 0.14248 |  1:21:46s\n",
      "\n",
      "Early stopping occurred at epoch 45 with best_epoch = 40 and best_valid_mae = 0.14167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    }
   ],
   "source": [
    "best_params = {\n",
    "    'n_d': 42, \n",
    "    \"n_a\": 48,  # n_a는 n_d와 동일하게 설정\n",
    "    'n_steps': 3,\n",
    "    'gamma': 1.9699098521619942, \n",
    "    'lambda_sparse': 0.0019000671753502962, \n",
    "    'optimizer_params': {\"lr\" : 0.0026506405886809045},\n",
    "    \"n_independent\": 5, # 필요하면 3, 4로 늘려본다\n",
    "    \"n_shared\": 2, # 필요하면 3, 4로 늘려본다\n",
    "    \"optimizer_fn\": torch.optim.Adam,\n",
    "    \"verbose\": 1,\n",
    "    \"device_name\" : \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    \"seed\" : 42\n",
    "}\n",
    "best_model = TabNetRegressor(**best_params)\n",
    "best_model.fit(\n",
    "            X_train.values, y_train.values.reshape(-1, 1),\n",
    "            eval_set=[(X_train.values, y_train.values.reshape(-1, 1)),(X_val.values, y_val.values.reshape(-1, 1))],\n",
    "            eval_name=['train', 'valid'],\n",
    "            eval_metric=[\"mae\"],\n",
    "            loss_fn=torch.nn.L1Loss(),\n",
    "            max_epochs=200, \n",
    "            patience=5,\n",
    "            batch_size=8192,\n",
    "            drop_last=False,\n",
    "            warm_start=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model_train.py에 합친다면 ver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_params, mae = optuna_train(\"tabnet\", X, y)\n",
    "# best_model = set_model(\"tabnet\", **best_params)\n",
    "# best_model = best_model.fit(X.values, y[\"log_deposit\"].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "torch.backends.cuda.matmul.allow_tf32 = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interest_rate 컬럼의 결측값을 평균값으로 대체\n",
    "test_data[\"interest_rate\"].fillna(test_data[\"interest_rate\"].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test_data.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = best_model.predict(X_test)\n",
    "y_pred = np.expm1(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission[\"deposit\"] = y_pred\n",
    "sample_submission.to_csv(\"output.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_csv(best_model, test_data, sample_submission)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

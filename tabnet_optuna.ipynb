{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from data.load_dataset import load_dataset\n",
    "from data.merge_dataset import merge_dataset\n",
    "from data.feature_engineering import *\n",
    "from model.inference import save_csv\n",
    "from model.feature_select import select_features\n",
    "from model.data_split import split_features_and_target\n",
    "from model.log_transformation import apply_log_transformation\n",
    "from model.model_train import set_model, optuna_train\n",
    "#from model.TreeModel import XGBoost\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "import optuna\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기존 데이터 불러오기\n",
    "train_data, test_data, sample_submission, interest_data, subway_data, school_data, park_data = load_dataset()\n",
    "# 기존 데이터에 새로운 feature들을 병합한 데이터프레임 불러오기\n",
    "train_data, test_data = merge_dataset(train_data, test_data, interest_data, subway_data, school_data, park_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 위치 중복도 낮은 행 삭제\n",
    "groups = train_data.groupby([\"latitude\", \"longitude\"])[\"index\"].count()\n",
    "conditioned_groups_index = groups[(groups >= 2) & (groups <= 5)].index # 이 범위를 파라미터로 조정하는걸로\n",
    "small_groups = train_data[\n",
    "    train_data[\"latitude\"].isin(conditioned_groups_index.get_level_values(0)) &\n",
    "    train_data[\"longitude\"].isin(conditioned_groups_index.get_level_values(1))\n",
    "]\n",
    "train_data.drop(small_groups.index, axis=0, inplace=True)\n",
    "\n",
    "# built_year > 2024 행 삭제\n",
    "train_data = train_data[train_data[\"built_year\"] < 2024]\n",
    "train_data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Clustering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 준비\n",
    "feature_columns = [\"latitude\", \"longitude\"]\n",
    "coords = train_data[feature_columns]\n",
    "\n",
    "# ClusteringModel 클래스 인스턴스 생성\n",
    "clustering_model = ClusteringModel(data=coords)\n",
    "kmeans_model = clustering_model.kmeans_clustering(n_clusters=25, \n",
    "                                                train_data=train_data, \n",
    "                                                test_data=test_data, \n",
    "                                                feature_columns=feature_columns, \n",
    "                                                label_column=\"region\")\n",
    "\n",
    "region_mean_prices = train_data.groupby(\"region\")[\"deposit\"].mean().reset_index()\n",
    "region_mean_prices.columns = [\"region\", \"mean_deposit\"]\n",
    "region_mean_prices[\"mean_deposit_category\"] = region_mean_prices[\"mean_deposit\"] // 10000\n",
    "\n",
    "# train_data와 region_mean_prices 병합\n",
    "train_data = train_data.merge(region_mean_prices, on=\"region\", how=\"left\")\n",
    "test_data = test_data.merge(region_mean_prices, on=\"region\", how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Log변환**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = apply_log_transformation(train_data, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature select**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = select_features(train_data, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**train_data split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = split_features_and_target(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['log_area_m2', 'built_year', 'latitude', 'longitude',\n",
       "       'log_subway_distance', 'log_school_distance', 'log_park_distance',\n",
       "       'contract_year_month', 'contract_day'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['deposit', 'log_deposit'], dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Train and Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tabnet**\n",
    "- 테이블 데이터에서도 딥러닝이 잘 동작할 수 있게 만들어진 모델\n",
    "- 자동으로 중요한 features를 선택하기 떄문에 feature select부분은 제외"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model_train.py 따라한 ver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import mean_absolute_error\n",
    "# from sklearn.model_selection import KFold\n",
    "\n",
    "# def cv_train(model, X: pd.DataFrame, y: pd.DataFrame, verbose: bool = True) -> float:\n",
    "#     \"\"\"K-Fold를 이용하여 Cross Validation을 수행하는 함수입니다.\n",
    "\n",
    "#     Args:\n",
    "#         model: 수행하려는 모델\n",
    "#         X (pd.DataFrame): 독립 변수\n",
    "#         y (pd.DataFrame): 예측 변수. deposit과 log_deposit 열로 나뉨.\n",
    "#         verbose (bool, optional): Fold별 진행상황을 출력할지 여부. Defaults to True.\n",
    "\n",
    "#     Returns:\n",
    "#         float: 평균 MAE\n",
    "#     \"\"\"\n",
    "#     cv = 5\n",
    "#     kfold = KFold(n_splits=cv, shuffle=True, random_state=42)\n",
    "\n",
    "#     mae_list = []\n",
    "#     for i, (train_idx, valid_idx) in enumerate(kfold.split(X, y), start=1):\n",
    "#         if verbose: print(f\"training...[{i}/{cv}]\")\n",
    "\n",
    "#         X_train, y_train = X.loc[train_idx, :].values, y.loc[train_idx, \"log_deposit\"].values.reshape(-1, 1)\n",
    "#         X_valid, y_valid = X.loc[valid_idx, :].values, y.loc[valid_idx, \"deposit\"].values.reshape(-1, 1)\n",
    "\n",
    "#         model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], eval_metric=[\"mae\"])\n",
    "\n",
    "#         y_pred = model.predict(X_valid)\n",
    "#         y_pred = np.expm1(y_pred)\n",
    "#         fold_mae = mean_absolute_error(y_valid, y_pred)\n",
    "#         if verbose: print(f\"Valid MAE: {fold_mae:.4f}\")\n",
    "#         mae_list.append(fold_mae)\n",
    "\n",
    "#     mae = np.mean(mae_list)\n",
    "#     if verbose:\n",
    "#         print(\"### K-fold Result ###\")\n",
    "#         print(f\"Valid MAE: {mae:.4f}\")\n",
    "    \n",
    "#     return mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def objective(trial):\n",
    "#     params = {\n",
    "#         \"n_d\": trial.suggest_int(\"n_d\", 16, 64),\n",
    "#         \"n_steps\": trial.suggest_int(\"n_steps\", 3, 10),\n",
    "#         \"gamma\": trial.suggest_float(\"gamma\", 1.0, 2.0),\n",
    "#         \"lambda_sparse\": trial.suggest_float(\"lambda_sparse\", 0.0001, 0.01),\n",
    "#         \"optimizer_fn\": torch.optim.Adam,\n",
    "#         \"optimizer_params\": dict(lr=trial.suggest_float(\"learning_rate\", 0.01, 0.1)),\n",
    "#     }\n",
    "#     model = TabNetRegressor(**params)\n",
    "#     return cv_train(model, X, y, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "합친 ver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import mean_absolute_error\n",
    "# from sklearn.model_selection import KFold, cross_val_predict\n",
    "\n",
    "# def objective(trial):\n",
    "#     params = {\n",
    "#         \"n_d\": trial.suggest_int(\"n_d\", 8, 64),\n",
    "#         \"n_a\": trial.suggest_int(\"n_a\", 8, 64),\n",
    "#         \"n_steps\": trial.suggest_int(\"n_steps\", 3, 10),\n",
    "#         \"gamma\": trial.suggest_float(\"gamma\", 1.0, 2.0),\n",
    "#         \"lambda_sparse\": trial.suggest_float(\"lambda_sparse\", 0.0001, 0.01),\n",
    "#         \"optimizer_params\": dict(lr=trial.suggest_float(\"learning_rate\", 0.01, 0.1)),\n",
    "#     }\n",
    "    \n",
    "#     # K-Fold 교차 검증\n",
    "#     cv = 5\n",
    "#     kfold = KFold(n_splits=cv, shuffle=True, random_state=42)\n",
    "#     mae_list = []\n",
    "    \n",
    "#     for i, (train_idx, valid_idx) in enumerate(kfold.split(X, y), start=1):\n",
    "#         if True: print(f\"training...[{i}/{cv}]\")\n",
    "#         X_train, y_train = X.loc[train_idx, :].values, y.loc[train_idx, \"log_deposit\"].values.reshape(-1, 1)\n",
    "#         X_valid, y_valid = X.loc[valid_idx, :].values, y.loc[valid_idx, \"deposit\"].values.reshape(-1, 1)\n",
    "        \n",
    "#         model = TabNetRegressor(**params)\n",
    "#         # 모델 학습 (patience : 성능 개선되지않으면 early stopping)\n",
    "#         model.fit(\n",
    "#             X_train, y_train, \n",
    "#             eval_set=[(X_valid, y_valid)], \n",
    "#             eval_metric=[\"mae\"], \n",
    "#             max_epochs=100,\n",
    "#             patience=10,\n",
    "#             batch_size=1024,\n",
    "#             virtual_batch_size=128,\n",
    "#         )\n",
    "        \n",
    "        \n",
    "#         # 검증 데이터에 대한 예측\n",
    "#         y_pred = model.predict(X_valid.values)\n",
    "#         y_pred = np.expm1(y_pred)  # 로그 변환의 역변환\n",
    "        \n",
    "#         # MAE 계산\n",
    "#         mae = mean_absolute_error(y_valid, y_pred) \n",
    "#         mae_list.append(mae)\n",
    "\n",
    "#     # 교차 검증 후 MAE 평균값 반환\n",
    "#     return np.mean(mae_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.metrics import mean_absolute_error\n",
    "# from sklearn.model_selection import KFold\n",
    "# from model.TreeModel import XGBoost, LightGBM, CatBoost\n",
    "# import optuna\n",
    "# RANDOM_SEED = 42\n",
    "\n",
    "# def set_model(model_name: str, **params):\n",
    "#     \"\"\"주어진 모델 이름에 따라 모델을 생성하고 반환하는 함수입니다.\n",
    "\n",
    "#     Args:\n",
    "#         model_name (str): 생성하려는 모델 이름\n",
    "#         **params (dict): 모델 생성 시 사용할 하이퍼파라미터\n",
    "\n",
    "#     Returns:\n",
    "#         model (object): 생성된 모델 객체\n",
    "#     \"\"\"\n",
    "#     match model_name:\n",
    "#         case \"xgboost\":\n",
    "#             model = XGBoost(**params)\n",
    "#         case \"lightgbm\":\n",
    "#             model = LightGBM(**params)\n",
    "#         case \"catboost\":\n",
    "#             model = CatBoost(**params)\n",
    "#         case \"tabnet\":\n",
    "#             model = TabNetRegressor(**params)\n",
    "#     return model\n",
    "\n",
    "# def cv_train(model, X: pd.DataFrame, y: pd.DataFrame, verbose: bool = True) -> float:\n",
    "#     \"\"\"K-Fold를 이용하여 Cross Validation을 수행하는 함수입니다.\n",
    "\n",
    "#     Args:\n",
    "#         model: 수행하려는 모델\n",
    "#         X (pd.DataFrame): 독립 변수\n",
    "#         y (pd.DataFrame): 예측 변수. deposit과 log_deposit 열로 나뉨.\n",
    "#         verbose (bool, optional): Fold별 진행상황을 출력할지 여부. Defaults to True.\n",
    "\n",
    "#     Returns:\n",
    "#         float: 평균 MAE\n",
    "#     \"\"\"\n",
    "#     cv = 5\n",
    "#     kfold = KFold(n_splits=cv, shuffle=True, random_state=42)\n",
    "\n",
    "#     mae_list = []\n",
    "#     for i, (train_idx, valid_idx) in enumerate(kfold.split(X, y), start=1):\n",
    "#         if verbose: print(f\"training...[{i}/{cv}]\")\n",
    "\n",
    "#         X_train, y_train = X.loc[train_idx, :].values, y.loc[train_idx, \"log_deposit\"].values.reshape(-1,1)\n",
    "#         X_valid, y_valid = X.loc[valid_idx, :].values, y.loc[valid_idx, \"deposit\"].values.reshape(-1,1)\n",
    "\n",
    "#         model.fit(\n",
    "#             X_train, y_train, \n",
    "#             eval_set=[(X_valid, y_valid)], \n",
    "#             eval_metric=[\"mae\"], \n",
    "#             max_epochs=100,\n",
    "#             patience=10\n",
    "#         )\n",
    "\n",
    "#         y_pred = model.predict(X_valid)\n",
    "#         y_pred = np.expm1(y_pred)\n",
    "#         fold_mae = mean_absolute_error(y_valid, y_pred)\n",
    "#         if verbose: print(f\"Valid MAE: {fold_mae:.4f}\")\n",
    "#         mae_list.append(fold_mae)\n",
    "\n",
    "#     mae = np.mean(mae_list)\n",
    "#     if verbose:\n",
    "#         print(\"### K-fold Result ###\")\n",
    "#         print(f\"Valid MAE: {mae:.4f}\")\n",
    "    \n",
    "#     return mae\n",
    "\n",
    "# def optuna_train(model_name: str, X: pd.DataFrame, y: pd.DataFrame) -> tuple[dict, float]:\n",
    "#     \"\"\"Optuna를 사용하여 주어진 모델의 하이퍼파라미터를 최적하는 함수\n",
    "\n",
    "#     Args:\n",
    "#         model_name (str): 최적화할 모델의 이름\n",
    "#         X (pd.DataFrame): 독립 변수\n",
    "#         y (pd.DataFrame): 예측 변수\n",
    "\n",
    "#     Returns:\n",
    "#         tuple[dict, float]:\n",
    "#             - dict: 최적의 하이퍼파라미터\n",
    "#             - float: 최적의 하이퍼파라미터에 대한 성능 지표(MAE)\n",
    "#     \"\"\"\n",
    "#     def objective(trial):\n",
    "#         match model_name:\n",
    "#             case \"xgboost\":\n",
    "#                 params = {\n",
    "#                     \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 300),\n",
    "#                     \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.2),\n",
    "#                     \"max_depth\": trial.suggest_int(\"max_depth\", 5, 12),\n",
    "#                     \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "#                 }\n",
    "#             case \"lightgbm\":\n",
    "#                 params = {\n",
    "#                     \"verbose\": -1,\n",
    "#                     \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 300),\n",
    "#                     \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True),\n",
    "#                     \"max_depth\": trial.suggest_int(\"max_depth\", 5, 12),\n",
    "#                     \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "#                     \"num_leaves\": trial.suggest_int(\"num_leaves\", 20, 150),\n",
    "#                     \"objective\": \"regression_l1\"\n",
    "#             }\n",
    "#             case \"catboost\":\n",
    "#                 params = {\n",
    "#                     \"verbose\": 0,\n",
    "#                     \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n",
    "#                     \"iterations\": trial.suggest_int(\"iterations\", 50, 500),\n",
    "#                     \"depth\": trial.suggest_int(\"depth\", 3, 10),\n",
    "#                     \"l2_leaf_reg\": trial.suggest_int(\"l2_leaf_reg\", 1, 10),\n",
    "#                     # \"bagging_temperature\": trial.suggest_loguniform(\"bagging_temperature\", 0.01, 1),\n",
    "#                     # \"border_count\": trial.suggest_int(\"border_count\", 32, 255),\n",
    "#                     \"cat_features\": [\"contract_day\"],\n",
    "#                     \"task_type\": \"GPU\",\n",
    "#                     \"devices\": \"cuda\",\n",
    "#                 }\n",
    "#             case \"tabnet\":\n",
    "#                 params = {\n",
    "#                     \"n_d\": trial.suggest_int(\"n_d\", 8, 64),\n",
    "#                     \"n_steps\": trial.suggest_int(\"n_steps\", 3, 10),\n",
    "#                     \"gamma\": trial.suggest_float(\"gamma\", 1.0, 2.0),\n",
    "#                     \"lambda_sparse\": trial.suggest_float(\"lambda_sparse\", 0.0001, 0.01),\n",
    "#                     \"optimizer_params\": dict(lr=trial.suggest_float(\"learning_rate\", 0.01, 0.1)),\n",
    "#                 }\n",
    "#         model = set_model(model_name, **params)\n",
    "#         return cv_train(model, X, y, verbose=False)\n",
    "    \n",
    "#     sampler = optuna.samplers.TPESampler(seed=42)\n",
    "#     study = optuna.create_study(direction=\"minimize\", sampler=sampler)\n",
    "#     study.optimize(objective, n_trials=50)\n",
    "#     return study.best_params, study.best_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "값이 너무 커서 hyperparameter 범위 조정 & kfold train,valid split부분 수정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from sklearn.metrics import mean_absolute_error\n",
    "# from sklearn.model_selection import KFold\n",
    "# import optuna\n",
    "# import torch\n",
    "# from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "\n",
    "# def cv_train(model, X: pd.DataFrame, y: pd.DataFrame, verbose: bool = True) -> float:\n",
    "#     \"\"\"K-Fold를 이용하여 Cross Validation을 수행하는 함수입니다.\n",
    "\n",
    "#     Args:\n",
    "#         model: 수행하려는 모델\n",
    "#         X (pd.DataFrame): 독립 변수\n",
    "#         y (pd.DataFrame): 예측 변수. deposit과 log_deposit 열로 나뉨.\n",
    "#         verbose (bool, optional): Fold별 진행상황을 출력할지 여부. Defaults to True.\n",
    "\n",
    "#     Returns:\n",
    "#         float: 평균 MAE\n",
    "#     \"\"\"\n",
    "#     cv = 5\n",
    "#     kfold = KFold(n_splits=cv, shuffle=True, random_state=42)\n",
    "\n",
    "#     mae_list = []\n",
    "#     for i, (train_idx, valid_idx) in enumerate(kfold.split(X, y), start=1):\n",
    "#         if verbose: print(f\"training...[{i}/{cv}]\")\n",
    "\n",
    "#         X_train, y_train = X.iloc[train_idx, :].values, y.iloc[train_idx, 0].values.reshape(-1, 1)\n",
    "#         X_valid, y_valid = X.iloc[valid_idx, :].values, y.iloc[valid_idx, 1].values.reshape(-1, 1)\n",
    "\n",
    "\n",
    "#         model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], eval_metric=[\"mae\"])\n",
    "\n",
    "#         y_pred = model.predict(X_valid)\n",
    "#         y_pred = np.expm1(y_pred)  # 로그 변환 복구\n",
    "        \n",
    "#         fold_mae = mean_absolute_error(y_valid, y_pred)\n",
    "#         if verbose: print(f\"Valid MAE: {fold_mae:.4f}\")\n",
    "#         mae_list.append(fold_mae)\n",
    "\n",
    "#     mae = np.mean(mae_list)\n",
    "#     if verbose:\n",
    "#         print(\"### K-fold Result ###\")\n",
    "#         print(f\"Valid MAE: {mae:.4f}\")\n",
    "    \n",
    "#     return mae\n",
    "\n",
    "# def objective(trial):\n",
    "#     params = {\n",
    "#         \"n_d\": trial.suggest_int(\"n_d\", 8, 64),\n",
    "#         'n_a': trial.suggest_int('n_a', 8, 64),\n",
    "#         \"n_steps\": trial.suggest_int(\"n_steps\", 3, 10),\n",
    "#         \"gamma\": trial.suggest_float(\"gamma\", 1.0, 2.0),\n",
    "#         'n_independent': trial.suggest_int('n_independent', 1, 5),\n",
    "#         'n_shared': trial.suggest_int('n_shared', 1, 5),\n",
    "#         \"lambda_sparse\": trial.suggest_float(\"lambda_sparse\", 0.0001, 0.01),\n",
    "#         \"optimizer_fn\": torch.optim.Adam,\n",
    "#         \"optimizer_params\": dict(lr=trial.suggest_float(\"learning_rate\", 0.001, 0.01)),\n",
    "#     }\n",
    "#     model = TabNetRegressor(**params)\n",
    "#     return cv_train(model, X, y, verbose=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampler = optuna.samplers.TPESampler(seed=42)\n",
    "# study = optuna.create_study(direction=\"minimize\", sampler=sampler)\n",
    "# study.optimize(objective, n_trials=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kfold 제거 ver 그러나 실패"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from sklearn.metrics import mean_absolute_error\n",
    "# import optuna\n",
    "# import torch\n",
    "# from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "\n",
    "# def train_model(model, X: pd.DataFrame, y: pd.DataFrame) -> float:\n",
    "#     \"\"\"모델을 학습하고 검증 MAE를 계산하는 함수입니다.\n",
    "\n",
    "#     Args:\n",
    "#         model: 수행하려는 모델\n",
    "#         X (pd.DataFrame): 독립 변수\n",
    "#         y (pd.DataFrame): 예측 변수. deposit과 log_deposit 열로 나뉨.\n",
    "\n",
    "#     Returns:\n",
    "#         float: 검증 MAE\n",
    "#     \"\"\"\n",
    "#     # 모델 학습\n",
    "#     model.fit(X.values, y.iloc[:, 0].values.reshape(-1, 1), eval_metric=[\"mae\"])\n",
    "\n",
    "#     # 예측 및 로그 변환 복구\n",
    "#     y_pred = model.predict(X.values)\n",
    "#     y_pred = np.expm1(y_pred)  # log_deposit의 inverse log 처리\n",
    "\n",
    "#     # 검증 MAE 계산\n",
    "#     mae = mean_absolute_error(y.iloc[:, 1], y_pred)  # deposit 열을 사용\n",
    "#     return mae\n",
    "\n",
    "# def objective(trial):\n",
    "#     \"\"\"Optuna를 이용하여 Hyperparameter 튜닝을 수행하는 함수입니다.\"\"\"\n",
    "#     params = {\n",
    "#         \"n_d\": trial.suggest_int(\"n_d\", 8, 64),\n",
    "#         \"n_a\": trial.suggest_int(\"n_a\", 8, 64),\n",
    "#         \"n_steps\": trial.suggest_int(\"n_steps\", 3, 10),\n",
    "#         \"gamma\": trial.suggest_float(\"gamma\", 1.0, 2.0),\n",
    "#         \"n_independent\": trial.suggest_int(\"n_independent\", 1, 5),\n",
    "#         \"n_shared\": trial.suggest_int(\"n_shared\", 1, 5),\n",
    "#         \"lambda_sparse\": trial.suggest_float(\"lambda_sparse\", 0.0001, 0.01),\n",
    "#         \"optimizer_fn\": torch.optim.Adam,\n",
    "#         \"optimizer_params\": dict(lr=trial.suggest_float(\"learning_rate\", 0.001, 0.01)),\n",
    "#     }\n",
    "\n",
    "#     # TabNet 모델 생성\n",
    "#     model = TabNetRegressor(**params)\n",
    "    \n",
    "#     # 모델 학습 및 MAE 계산\n",
    "#     mae = train_model(model, X, y)\n",
    "    \n",
    "#     return mae\n",
    "\n",
    "# # Optuna 실험 세팅 및 실행\n",
    "# sampler = optuna.samplers.TPESampler(seed=42)\n",
    "# study = optuna.create_study(direction=\"minimize\", sampler=sampler)\n",
    "# study.optimize(objective, n_trials=50)\n",
    "\n",
    "# # 최적 하이퍼파라미터 출력\n",
    "# best_params = study.best_params\n",
    "# print(\"Best hyperparameters: \", best_params)\n",
    "# print(\"Best MAE: \", study.best_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kfold 없애고 max_epoch=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-22 10:52:29,440] A new study created in memory with name: no-name-8c5f7f22-2c1b-45bb-80c0-cacb303091c2\n",
      "/opt/conda/lib/python3.10/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cuda\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.5919  | val_0_mae: 0.29748 |  0:01:38s\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "def train_model(model, X: pd.DataFrame, y: pd.DataFrame) -> float:\n",
    "    \"\"\"모델을 학습하고 검증 MAE를 계산하는 함수입니다.\n",
    "\n",
    "    Args:\n",
    "        model: 수행하려는 모델\n",
    "        X (pd.DataFrame): 독립 변수\n",
    "        y (pd.DataFrame): 예측 변수. deposit과 log_deposit 열로 나뉨.\n",
    "\n",
    "    Returns:\n",
    "        float: 검증 MAE\n",
    "    \"\"\"\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y['log_deposit'], test_size=0.2, random_state=42)\n",
    " \n",
    "    # 모델 학습\n",
    "    model.fit(\n",
    "        X_train.values, y_train.values.reshape(-1, 1),\n",
    "        eval_set=[(X_val.values, y_val.values.reshape(-1, 1))],\n",
    "        eval_metric=[\"mae\"],\n",
    "        max_epochs=100,\n",
    "        patience=10,\n",
    "        batch_size=1024,\n",
    "        virtual_batch_size=128,\n",
    "        num_workers=0,\n",
    "        drop_last=False,\n",
    "    )\n",
    "\n",
    "    # 예측 및 로그 변환 복구\n",
    "    y_pred_log = model.predict(X_val.values)\n",
    "    y_pred = np.expm1(y_pred_log)  # log_deposit의 inverse log 처리\n",
    "\n",
    "    # 검증 MAE 계산\n",
    "    mae = mean_absolute_error(y[\"deposit\"].values, y_pred)  # deposit 열을 사용\n",
    "    return mae\n",
    "\n",
    "def objective(trial):\n",
    "    \"\"\"Optuna를 이용하여 Hyperparameter 튜닝을 수행하는 함수입니다.\"\"\"\n",
    "    params = {\n",
    "        \"n_d\": trial.suggest_int(\"n_d\", 8, 64),\n",
    "        \"n_a\": trial.suggest_int(\"n_a\", 8, 64),\n",
    "        \"n_steps\": trial.suggest_int(\"n_steps\", 3, 10),\n",
    "        \"gamma\": trial.suggest_float(\"gamma\", 1.0, 2.0),\n",
    "        \"n_independent\": trial.suggest_int(\"n_independent\", 1, 5),\n",
    "        \"n_shared\": trial.suggest_int(\"n_shared\", 1, 5),\n",
    "        \"lambda_sparse\": trial.suggest_float(\"lambda_sparse\", 0.0001, 0.01),\n",
    "        \"optimizer_fn\": torch.optim.Adam,\n",
    "        \"optimizer_params\": dict(lr=trial.suggest_float(\"learning_rate\", 0.001, 0.01)),\n",
    "        \"device_name\" : \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    }\n",
    "\n",
    "    # TabNet 모델 생성\n",
    "    model = TabNetRegressor(**params)\n",
    "\n",
    "\n",
    "    \n",
    "    # 모델 학습 및 MAE 계산\n",
    "    mae = train_model(model, X, y)\n",
    "    print(f\"Trial {trial.number}: MAE = {mae}\")\n",
    "    \n",
    "    return mae\n",
    "\n",
    "# Optuna 실험 세팅 및 실행\n",
    "sampler = optuna.samplers.TPESampler(seed=42)\n",
    "study = optuna.create_study(direction=\"minimize\", sampler=sampler)\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# 최적 하이퍼파라미터 출력\n",
    "best_params = study.best_params\n",
    "print(\"Best hyperparameters: \", best_params)\n",
    "print(\"Best MAE: \", study.best_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_params = study.best_params\n",
    "# print(\"Best parameters for Tabnet: \", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = TabNetRegressor(**best_params)\n",
    "best_model.fit(X.values, y[\"log_deposit\"].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model_train.py에 합친다면 ver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_params, mae = optuna_train(\"tabnet\", X, y)\n",
    "# best_model = set_model(\"tabnet\", **best_params)\n",
    "# best_model = best_model.fit(X.values, y[\"log_deposit\"].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_csv(best_model, test_data, sample_submission)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
